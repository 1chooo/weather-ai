{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Weather and Artificial Intelligence","text":"<p>The course \"Weather and Artificial Intelligence\" at Dept. Atm, NCU.  </p> <p>Year: 2023-Fall Lecturer: Che-Wei Chou (\u5468\u54f2\u7dad) TA: Hugo ChunHo Lin (\u6797\u7fa4\u8cc0)</p>"},{"location":"#course-introduction","title":"Course Introduction","text":""},{"location":"#textbook","title":"Textbook","text":""},{"location":"contributing/","title":"Contributing","text":"<p>How to cintribute your effort to this repo?</p> <p>To be continued... Still Constructing...</p>"},{"location":"course_materials/","title":"Course Material","text":""},{"location":"course_materials/#2023-fall","title":"2023-Fall","text":""},{"location":"course_materials/#2024-spring","title":"2024-Spring","text":""},{"location":"env_test/","title":"Build Environment with Machine Learning","text":"<p>To create the virtual environment for this course, you have two methods available. The first method involves using pipenv, while the second method involves building with a Conda environment. If you choose the Conda method, please ensure that you have Conda or Miniconda installed on your system.</p> In\u00a0[\u00a0]: Copied! <pre># ## Example Program\n# Below is the program relating to the package we often use in Machine Learning to test the environment building successfully or not.\n</pre> # ## Example Program # Below is the program relating to the package we often use in Machine Learning to test the environment building successfully or not. In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf\nimport torch\nimport cv2\nimport nltk\n</pre> import matplotlib.pyplot as plt import pandas as pd from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression import keras from keras.models import Sequential from keras.layers import Dense import tensorflow as tf import torch import cv2 import nltk In\u00a0[\u00a0]: Copied! <pre># \u4f7f\u7528 sklearn \u5167\u5efa\u7684\u8cc7\u6599\u96c6\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n</pre> # \u4f7f\u7528 sklearn \u5167\u5efa\u7684\u8cc7\u6599\u96c6 iris = datasets.load_iris() X = iris.data y = iris.target In\u00a0[\u00a0]: Copied! <pre># \u5c07\u8cc7\u6599\u96c6\u5207\u5272\u6210\u8a13\u7df4\u96c6\u548c\u6e2c\u8a66\u96c6\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</pre> # \u5c07\u8cc7\u6599\u96c6\u5207\u5272\u6210\u8a13\u7df4\u96c6\u548c\u6e2c\u8a66\u96c6 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) In\u00a0[\u00a0]: Copied! <pre># \u4f7f\u7528 Logistic Regression \u5206\u985e\u5668\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n</pre> # \u4f7f\u7528 Logistic Regression \u5206\u985e\u5668 clf = LogisticRegression() clf.fit(X_train, y_train) In\u00a0[\u00a0]: Copied! <pre># \u4f7f\u7528 Matplotlib \u7e6a\u88fd\u8cc7\u6599\u548c\u5206\u985e\u7d50\u679c\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.title('Iris Data')\nplt.show()\n</pre> # \u4f7f\u7528 Matplotlib \u7e6a\u88fd\u8cc7\u6599\u548c\u5206\u985e\u7d50\u679c plt.scatter(X[:, 0], X[:, 1], c=y) plt.xlabel('Sepal Length') plt.ylabel('Sepal Width') plt.title('Iris Data') plt.show() In\u00a0[\u00a0]: Copied! <pre># \u4f7f\u7528 Pandas \u8b80\u53d6\u8cc7\u6599\u96c6\u4e26\u986f\u793a\u524d\u5e7e\u7b46\u8cc7\u6599\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\nprint(df.head())\n</pre> # \u4f7f\u7528 Pandas \u8b80\u53d6\u8cc7\u6599\u96c6\u4e26\u986f\u793a\u524d\u5e7e\u7b46\u8cc7\u6599 df = pd.DataFrame(iris.data, columns=iris.feature_names) print(df.head()) In\u00a0[\u00a0]: Copied! <pre># \u4f7f\u7528 Keras \u5efa\u7acb\u4e00\u500b\u7c21\u55ae\u7684\u795e\u7d93\u7db2\u8def\u6a21\u578b\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=4, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n</pre> # \u4f7f\u7528 Keras \u5efa\u7acb\u4e00\u500b\u7c21\u55ae\u7684\u795e\u7d93\u7db2\u8def\u6a21\u578b model = Sequential() model.add(Dense(10, input_dim=4, activation='relu')) model.add(Dense(3, activation='softmax')) In\u00a0[\u00a0]: Copied! <pre># \u4f7f\u7528 TensorFlow \u8a2d\u5b9a\u904b\u7b97\u914d\u7f6e\u4e26\u8a13\u7df4\u6a21\u578b\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\nkeras.backend.set_session(session)\n</pre> # \u4f7f\u7528 TensorFlow \u8a2d\u5b9a\u904b\u7b97\u914d\u7f6e\u4e26\u8a13\u7df4\u6a21\u578b config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config) keras.backend.set_session(session) In\u00a0[\u00a0]: Copied! <pre>model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=100, batch_size=10, verbose=1)\n</pre> model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=1) In\u00a0[\u00a0]: Copied! <pre># \u4f7f\u7528 PyTorch \u5efa\u7acb\u4e00\u500b\u7c21\u55ae\u7684\u795e\u7d93\u7db2\u8def\u6a21\u578b\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = torch.nn.Linear(4, 10)\n        self.fc2 = torch.nn.Linear(10, 3)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n</pre> # \u4f7f\u7528 PyTorch \u5efa\u7acb\u4e00\u500b\u7c21\u55ae\u7684\u795e\u7d93\u7db2\u8def\u6a21\u578b class Net(torch.nn.Module):     def __init__(self):         super(Net, self).__init__()         self.fc1 = torch.nn.Linear(4, 10)         self.fc2 = torch.nn.Linear(10, 3)      def forward(self, x):         x = torch.relu(self.fc1(x))         x = self.fc2(x)         return x In\u00a0[\u00a0]: Copied! <pre>net = Net()\n</pre> net = Net() In\u00a0[\u00a0]: Copied! <pre># \u8a2d\u5b9a\u904b\u7b97\u88dd\u7f6e\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnet.to(device)\n</pre> # \u8a2d\u5b9a\u904b\u7b97\u88dd\u7f6e device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") net.to(device) In\u00a0[\u00a0]: Copied! <pre># \u5b9a\u7fa9\u640d\u5931\u51fd\u6578\u548c\u512a\u5316\u5668\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n</pre> # \u5b9a\u7fa9\u640d\u5931\u51fd\u6578\u548c\u512a\u5316\u5668 criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(net.parameters(), lr=0.001) In\u00a0[\u00a0]: Copied! <pre># \u8f49\u63db\u8a13\u7df4\u8cc7\u6599\u70ba Tensor\ninputs = torch.Tensor(X_train).to(device)\nlabels = torch.Tensor(y_train).long().to(device)\n</pre> # \u8f49\u63db\u8a13\u7df4\u8cc7\u6599\u70ba Tensor inputs = torch.Tensor(X_train).to(device) labels = torch.Tensor(y_train).long().to(device) In\u00a0[\u00a0]: Copied! <pre># \u8a13\u7df4\u6a21\u578b\nfor epoch in range(100):\n    optimizer.zero_grad()\n    outputs = net(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\nprint(\"All packages are installed and working correctly!\")\n</pre> # \u8a13\u7df4\u6a21\u578b for epoch in range(100):     optimizer.zero_grad()     outputs = net(inputs)     loss = criterion(outputs, labels)     loss.backward()     optimizer.step()  print(\"All packages are installed and working correctly!\")"},{"location":"env_test/#build-environment-with-machine-learning","title":"Build Environment with Machine Learning\u00b6","text":""},{"location":"env_test/#install-the-python-interpreter","title":"Install the Python Interpreter\u00b6","text":""},{"location":"env_test/#create-the-virtual-environment","title":"Create the Virtual Environment\u00b6","text":"<ul> <li>Python Interpreter Request: <code>python3.7</code></li> <li>Package we need: numpy, matplotlib, pandas, scipy, scikit-learn, keras, tensorflow, torch, opencv-python, nltk, ipykernel</li> </ul>"},{"location":"env_test/#with-pip-vertual-environment","title":"With pip Vertual Environment\u00b6","text":"<pre>$ pip install --upgrade pip\n$ pip3 install virtualenv\n$ virtualenv venv --python=python3.7    # Create python3.7.x virtual environment\n$ source venv/bin/activate              # Activate the virtual environment\n\n# Check the environment status\n$ which python\n./venv/bin/python                       # in the venv/\n$ python --version\npython 3.7.16\n\n# Install the package we need\n$ pip install matplotlib\n$ pip install pandas\n$ pip install scikit-learn\n$ pip install keras\n$ pip install tensorflow\n$ pip install torch\n$ pip install opencv-python\n$ pip install nltk\n$ pip install ipykernel\n\n$ deactivate                            # Deactivate the virtual environment\n$ rm -rf venv                           # Remove the venv if you don't need\n</pre>"},{"location":"env_test/#with-conda-vertual-environment","title":"With Conda Vertual Environment\u00b6","text":"<pre>$ conda create --name py37 python=3.7\n$ conda activate py37\n\n$ conda install tensorflow=1.15.0\n$ conda install keras=2.3.1\n$ conda install matplotlib\n$ conda install numpy\n$ conda install opencv-python\n$ conda install torch\n$ conda install nltk\n$ conda install ipykernel\n$ conda install scipy\n$ conda install pandas\n</pre>"},{"location":"env_test/#python-code-to-test-virtual-environment","title":"Python Code to Test virtual Environment\u00b6","text":"<pre>pip install --upgrade pip\npip install matplotlib\npip install pandas\npip install scikit-learn\npip install keras\npip install tensorflow\npip install torch\npip install opencv-python\npip install nltk\n\npip install mkdocs\npip install mkdocs-material\npip install pymdown-extensions\npip install mkdocstrings\npip install mkdocs-git-revision-date-plugin\npip install mkdocs-jupyter\n</pre>"},{"location":"reference/","title":"Reference","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/","title":"HW01 CIFAR-10 with CNN","text":"In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom keras.datasets import cifar10\nfrom keras import models\nfrom keras import layers\n\n# Load CIFAR-10 dataset\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# Normalize pixel values between 0 and 1\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n</pre> import tensorflow as tf from keras.datasets import cifar10 from keras import models from keras import layers  # Load CIFAR-10 dataset (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()  # Normalize pixel values between 0 and 1 train_images, test_images = train_images / 255.0, test_images / 255.0 In\u00a0[\u00a0]: Copied! <pre>def build_model():\n\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=(32, 32, 3)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(10)\n    ])\n\n    model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n    \n    return model\n</pre> def build_model():      model = tf.keras.Sequential([         tf.keras.layers.Input(shape=(32, 32, 3)),         tf.keras.layers.Flatten(),         tf.keras.layers.Dense(32, activation='relu'),         tf.keras.layers.Dense(64, activation='relu'),         tf.keras.layers.Dense(64, activation='relu'),         tf.keras.layers.Dense(10)     ])      model.compile(optimizer='adam',               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=['accuracy'])          return model In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt # pip install matplotlib\nfrom random import randrange\n\ntext = [\n  'airplane', \n  'automobile',\n  'bird' ,\n  'cat', \n  'deer', \n  'dog', \n  'frog', \n  'horse', \n  'ship', \n  'truck'\n]\nplt.figure(figsize=(16,10),facecolor='w')\nfor i in range(5):\n  for j in range(8):\n    index = randrange(0, 50000)\n    plt.subplot(5, 8, i * 8 + j + 1)\n    plt.title(\"label: {}\".format(text[train_labels[index][0]]))\n    plt.imshow(train_images[index])\n    plt.axis('off')\n\nplt.show()\n</pre> import matplotlib.pyplot as plt # pip install matplotlib from random import randrange  text = [   'airplane',    'automobile',   'bird' ,   'cat',    'deer',    'dog',    'frog',    'horse',    'ship',    'truck' ] plt.figure(figsize=(16,10),facecolor='w') for i in range(5):   for j in range(8):     index = randrange(0, 50000)     plt.subplot(5, 8, i * 8 + j + 1)     plt.title(\"label: {}\".format(text[train_labels[index][0]]))     plt.imshow(train_images[index])     plt.axis('off')  plt.show() In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=20, \n    validation_data=(test_images, test_labels)\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=20,      validation_data=(test_images, test_labels) ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>history_dict = history.history\nhistory_dict.keys()\n</pre> history_dict = history.history history_dict.keys() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() <p>We find that about seven times, we can get the lowest <code>val_loss</code>, then we can re-compile our model again, and set the <code>epoches</code> to 7 or 8.</p> In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=7, \n    validation_data=(test_images, test_labels),\n    batch_size=800\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=7,      validation_data=(test_images, test_labels),     batch_size=800 ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>history_dict = history.history\nhistory_dict.keys()\n</pre> history_dict = history.history history_dict.keys() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=7, \n    validation_data=(test_images, test_labels),\n    batch_size=50\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=7,      validation_data=(test_images, test_labels),     batch_size=50 ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>def build_model():\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=(32, 32, 3)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(10)\n    ])\n\n    model.compile(optimizer='rmsprop',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n    \n    return model\n</pre> def build_model():          model = tf.keras.Sequential([         tf.keras.layers.Input(shape=(32, 32, 3)),         tf.keras.layers.Flatten(),         tf.keras.layers.Dense(32, activation='relu'),         tf.keras.layers.Dense(64, activation='relu'),         tf.keras.layers.Dense(64, activation='relu'),         tf.keras.layers.Dense(10)     ])      model.compile(optimizer='rmsprop',               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=['accuracy'])          return model In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=10, \n    validation_data=(test_images, test_labels),\n    batch_size=50\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=10,      validation_data=(test_images, test_labels),     batch_size=50 ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#hw01-cifar-10-with-cnn","title":"HW01 CIFAR-10 with CNN\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#course-ap4064","title":"Course: AP4064\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#assignment-1","title":"Assignment: 1\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#major-atm","title":"Major: ATM\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#name-hugo-chunho-lin","title":"Name: Hugo ChunHo Lin\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#student-id-109601003","title":"Student Id: 109601003\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#import-the-package-and-the-dataset","title":"Import the package and the dataset\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#define-the-dnn-model-architecture","title":"Define the DNN model architecture\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#visualize-the-data-type-and-the-label","title":"Visualize the data type and the label!\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#the-result-of-the-test-accuracy-truely-increases","title":"The result of the test accuracy truely increases!\u00b6","text":"<p>Then we can try other variables to improve the result of our model.</p>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#now-we-add-the-bastch_size","title":"Now we add the <code>bastch_size</code>\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#we-find-that-when-we-adjust-our-batch_size-we-can-also-increase-the-result-of-our-model","title":"We find that when we adjust our <code>batch_size</code>, we can also increase the result of our model!\u00b6","text":"<p>Then we can try another test to change our <code>optimizer</code>.</p>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#my-gained-knowledge","title":"My gained knowledge\u00b6","text":"<p>I have tested a lot of experiment to improve the model; however, all of the results still surround to about 0.70 accuracy.</p>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#the-best-accuracy-of-my-work-test-accuracy-0718500018119812","title":"The best accuracy of my work: <code>Test accuracy: 0.718500018119812</code>\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#below-are-the-experiments-i-have-conducted","title":"Below are the experiments I have conducted:\u00b6","text":"<ul> <li><code>epoches: from 20 -&gt; 7</code>.</li> <li><code>batch_size: from 100 -&gt; 50</code>.</li> <li><code>optimizer: adam and rmsprop</code>.</li> </ul> <p>Even though I designed a lot of experiments, the accuracy did not increase significantly. I have considered the reasons, and here are my conclusions.</p> <p>First, our Deep Neural-Network model was limited by the size of the CIFAR-10 dataset, which consisted of up to 50000 training_data and up to 10000 testing_data. The larger datasets made it difficult for the DNN model to capture all the necessary values during training, which resulted in less accuracy even when we changed several variables.</p> <p>Second, given the large amount of data, I could have tried to drop out the data that affected the results. However, I thought that we might be able to choose the Convolutional Neural-Network instead because it was more suitable for dropping out the worse neural in our model.</p> <p>In conclusion, I am excited to have the opportunity to improve my deep-learning skills with this dataset and to review what I have learned before.</p>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_CNN/#reference","title":"Reference\u00b6","text":"<ul> <li>Day 20 ~ AI\u5f9e\u5165\u9580\u5230\u653e\u68c4 - \u65b0\u7684\u8cc7\u6599\u96c6</li> <li>\u7c21\u55ae\u4f7f\u7528keras \u67b6\u69cb\u6df1\u5ea6\u5b78\u7fd2\u795e\u7d93\u7db2\u8def \u2014 \u4ee5cifar10\u70ba\u4f8b</li> </ul>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/","title":"HW01 CIFAR-10 with DNN","text":"In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom keras.datasets import cifar10\nfrom keras import models\nfrom keras import layers\n\n# Load CIFAR-10 dataset\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# Normalize pixel values between 0 and 1\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n</pre> import tensorflow as tf from keras.datasets import cifar10 from keras import models from keras import layers  # Load CIFAR-10 dataset (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()  # Normalize pixel values between 0 and 1 train_images, test_images = train_images / 255.0, test_images / 255.0 In\u00a0[\u00a0]: Copied! <pre>def build_model():\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=(32, 32, 3)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(10)\n    ])\n\n    model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n    \n    return model\n</pre> def build_model():          model = tf.keras.Sequential([         tf.keras.layers.Input(shape=(32, 32, 3)),         tf.keras.layers.Flatten(),         tf.keras.layers.Dense(32, activation='relu'),         tf.keras.layers.Dense(64, activation='relu'),         tf.keras.layers.Dense(64, activation='relu'),         tf.keras.layers.Dense(10)     ])      model.compile(optimizer='adam',               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=['accuracy'])          return model In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt # pip install matplotlib\nfrom random import randrange\n\ntext = [\n  'airplane', \n  'automobile',\n  'bird' ,\n  'cat', \n  'deer', \n  'dog', \n  'frog', \n  'horse', \n  'ship', \n  'truck'\n]\nplt.figure(figsize=(16,10),facecolor='w')\nfor i in range(5):\n  for j in range(8):\n    index = randrange(0, 50000)\n    plt.subplot(5, 8, i * 8 + j + 1)\n    plt.title(\"label: {}\".format(text[train_labels[index][0]]))\n    plt.imshow(train_images[index])\n    plt.axis('off')\n\nplt.show()\n</pre> import matplotlib.pyplot as plt # pip install matplotlib from random import randrange  text = [   'airplane',    'automobile',   'bird' ,   'cat',    'deer',    'dog',    'frog',    'horse',    'ship',    'truck' ] plt.figure(figsize=(16,10),facecolor='w') for i in range(5):   for j in range(8):     index = randrange(0, 50000)     plt.subplot(5, 8, i * 8 + j + 1)     plt.title(\"label: {}\".format(text[train_labels[index][0]]))     plt.imshow(train_images[index])     plt.axis('off')  plt.show() In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=20, \n    validation_data=(test_images, test_labels)\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=20,      validation_data=(test_images, test_labels) ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>history_dict = history.history\nhistory_dict.keys()\n</pre> history_dict = history.history history_dict.keys() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() <p>We find that about seven times, we can get the lowest <code>val_loss</code>, then we can re-compile our model again, and set the <code>epoches</code> to 14.</p> In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=14, \n    validation_data=(test_images, test_labels),\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=14,      validation_data=(test_images, test_labels), ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=7, \n    validation_data=(test_images, test_labels),\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=7,      validation_data=(test_images, test_labels), ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) <p>We still pickup the <code>epoches</code> to 14.</p> In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=14, \n    validation_data=(test_images, test_labels),\n    batch_size=100\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=14,      validation_data=(test_images, test_labels),     batch_size=100 ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=9, \n    validation_data=(test_images, test_labels),\n    batch_size=100\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=9,      validation_data=(test_images, test_labels),     batch_size=100 ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=9, \n    validation_data=(test_images, test_labels),\n    batch_size=500\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=9,      validation_data=(test_images, test_labels),     batch_size=500 ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) <p>Then we can try another test to change our <code>optimizer</code>.</p> In\u00a0[\u00a0]: Copied! <pre>def build_model():\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=(32, 32, 3)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(10)\n    ])\n\n    model.compile(optimizer='rmsprop',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n    \n    return model\n</pre> def build_model():          model = tf.keras.Sequential([         tf.keras.layers.Input(shape=(32, 32, 3)),         tf.keras.layers.Flatten(),         tf.keras.layers.Dense(32, activation='relu'),         tf.keras.layers.Dense(64, activation='relu'),         tf.keras.layers.Dense(64, activation='relu'),         tf.keras.layers.Dense(10)     ])      model.compile(optimizer='rmsprop',               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=['accuracy'])          return model In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=14, \n    validation_data=(test_images, test_labels),\n    batch_size=500\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=14,      validation_data=(test_images, test_labels),     batch_size=500 ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#hw01-cifar-10-with-dnn","title":"HW01 CIFAR-10 with DNN\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#course-ap4064","title":"Course: AP4064\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#assignment-1","title":"Assignment: 1\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#major-atm","title":"Major: ATM\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#name-hugo-chunho-lin","title":"Name: Hugo ChunHo Lin\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#student-id-109601003","title":"Student Id: 109601003\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#import-the-package-and-the-dataset","title":"Import the package and the dataset\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#define-the-dnn-model-architecture","title":"Define the DNN model architecture\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#visualize-the-data-type-and-the-label","title":"Visualize the data type and the label!\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#test-accuracy-040779998898506165","title":"Test accuracy: 0.40779998898506165\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#test-accuracy-040549999475479126","title":"Test accuracy: 0.40549999475479126\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#the-result-of-the-test-not-change-not-much-and-we-set-epoches-to-7","title":"The result of the test not change not much. And we set <code>epoches</code> to 7.\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#the-result-of-the-test-not-change-a-lot","title":"The result of the test not change a lot!\u00b6","text":"<p>Then we can try other variables to improve the result of our model.</p>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#now-we-add-the-bastch_size","title":"Now we add the <code>bastch_size</code>\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#the-result-of-the-test-truly-increases-then-we-can-find-it-may-occur-overfitting-at-epoches-9-then-we-change-the-epoches","title":"The result of the test truly increases! Then we can find it may occur overfitting at <code>epoches 9</code> then we change the epoches\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#however-the-accuracy-decrease-then-we-still-set-our-epoches-to-14-now-we-try-to-increase-the-speed-of-training-we-set-the-batch_size-larger","title":"However, the accuracy decrease, then we still set our <code>epoches</code> to 14. Now we try to increase the speed of training, we set the <code>batch_size</code> larger.\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#we-find-that-when-we-adjust-our-batch_size-larger-we-can-also-increase-the-result-of-our-model","title":"We find that when we adjust our <code>batch_size</code> larger, we can also increase the result of our model!\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#test-accuracy-04408000111579895","title":"Test accuracy: 0.4408000111579895\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#test-accuracy-04668000042438507","title":"Test accuracy: 0.4668000042438507\u00b6","text":"<p>And we found that the <code>val_loss</code> and <code>loss</code> more smoothly.</p>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#my-gained-knowledge","title":"My gained knowledge\u00b6","text":"<p>I have tested a lot of experiment to improve the model; however, all of the results still surround to about forty percent accuracy.</p>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#below-are-the-experiments-i-have-conducted","title":"Below are the experiments I have conducted:\u00b6","text":"<ul> <li><code>epoches: from 20 -&gt; 14 -&gt; 9 -&gt; 14</code>.</li> <li><code>batch_size: from 100 -&gt; 500</code>.</li> <li><code>optimizer: adam and rmsprop</code>.</li> </ul> <p>Even though I designed a lot of experiments, the accuracy did not increase significantly. I have considered the reasons, and here are my conclusions.</p> <p>First, our Deep Neural-Network model was limited by the size of the CIFAR-10 dataset, which consisted of up to 50000 training_data and up to 10000 testing_data. The larger datasets made it difficult for the DNN model to capture all the necessary values during training, which resulted in less accuracy even when we changed several variables.</p> <p>Second, given the large amount of data, I could have tried to drop out the data that affected the results. However, I thought that we might be able to choose the Convolutional Neural-Network instead because it was more suitable for dropping out the worse neural in our model.</p> <p>In conclusion, I am excited to have the opportunity to improve my deep-learning skills with this dataset and to review what I have learned before.</p>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_DNN/#reference","title":"Reference\u00b6","text":"<ul> <li>Day 20 ~ AI\u5f9e\u5165\u9580\u5230\u653e\u68c4 - \u65b0\u7684\u8cc7\u6599\u96c6</li> <li>\u7c21\u55ae\u4f7f\u7528keras \u67b6\u69cb\u6df1\u5ea6\u5b78\u7fd2\u795e\u7d93\u7db2\u8def \u2014 \u4ee5cifar10\u70ba\u4f8b</li> </ul>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_byChatGPT/","title":"Write by ChatGPT","text":"In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom keras.datasets import cifar10\nfrom keras import models\nfrom keras import layers\n\n# Load CIFAR-10 dataset\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# Normalize pixel values between 0 and 1\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n\n# Define the DNN model architecture\n\ndef build_model():\n    \n    model = models.Sequential()\n    model.add(layers.Dense(\n        ))\n    model = models.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(10)\n    ])\n\n    model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n    \n    return model\n\nmodel = build_model()\n\n# Compile the model with Adam optimizer and sparse categorical cross-entropy loss\n\n\n# Train the model\nhistory = model.fit(train_images, train_labels, epochs=10, \n                    validation_data=(test_images, test_labels))\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint('Test accuracy:', test_acc)\n</pre> import tensorflow as tf from keras.datasets import cifar10 from keras import models from keras import layers  # Load CIFAR-10 dataset (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()  # Normalize pixel values between 0 and 1 train_images, test_images = train_images / 255.0, test_images / 255.0  # Define the DNN model architecture  def build_model():          model = models.Sequential()     model.add(layers.Dense(         ))     model = models.Sequential([         layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),         layers.MaxPooling2D((2, 2)),         layers.Conv2D(64, (3, 3), activation='relu'),         layers.MaxPooling2D((2, 2)),         layers.Conv2D(64, (3, 3), activation='relu'),         layers.Flatten(),         layers.Dense(64, activation='relu'),         layers.Dense(10)     ])      model.compile(optimizer='adam',               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=['accuracy'])          return model  model = build_model()  # Compile the model with Adam optimizer and sparse categorical cross-entropy loss   # Train the model history = model.fit(train_images, train_labels, epochs=10,                      validation_data=(test_images, test_labels))  # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Load CIFAR-10 dataset\ncifar10 = tf.keras.datasets.cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Data preprocessing\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Build the model\nmodel = tf.keras.Sequential([\n    layers.InputLayer(input_shape=(32, 32, 3)),\n    layers.Conv2D(32, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(128, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, epochs=10)\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\nprint('\\nTest accuracy:', test_acc)\n</pre> import tensorflow as tf from tensorflow.keras import layers  # Load CIFAR-10 dataset cifar10 = tf.keras.datasets.cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data()  # Data preprocessing x_train, x_test = x_train / 255.0, x_test / 255.0  # Build the model model = tf.keras.Sequential([     layers.InputLayer(input_shape=(32, 32, 3)),     layers.Conv2D(32, 3, padding='same', activation='relu'),     layers.MaxPooling2D(),     layers.Conv2D(64, 3, padding='same', activation='relu'),     layers.MaxPooling2D(),     layers.Conv2D(128, 3, padding='same', activation='relu'),     layers.MaxPooling2D(),     layers.Flatten(),     layers.Dense(256, activation='relu'),     layers.Dense(10, activation='softmax') ])  # Compile the model model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Train the model model.fit(x_train, y_train, epochs=10)  # Evaluate the model test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2) print('\\nTest accuracy:', test_acc)"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_byChatGPT/#write-by-chatgpt","title":"Write by ChatGPT\u00b6","text":"<p>The result is too low to use...</p>"},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_byChatGPT/#dnn","title":"DNN\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_cifar10_byChatGPT/#cnn","title":"CNN\u00b6","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_description/","title":"Description","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_description/#spring-2023","title":"Spring, 2023","text":""},{"location":"archive/2023_spring/hw01/23_spring_hw01_description/#hw01","title":"HW01","text":"<p>Please print all the interactive output without resorting to print, not only the last result. To do this, you should add the following code in Jupyter cell to the beginning of the file. <pre><code>from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n</code></pre> Cifar10 \u8cc7\u6599\u5373\u5305\u542b 6 \u842c\u7b46 32*32 \u4f4e\u89e3\u6790\u5ea6\u4e4b\u5f69\u8272\u5716\u7247\uff0c</p> <p>\u5176\u4e2d 5 \u842c\u7b46\u70ba\u8a13\u7df4\u96c6\uff1b1 \u842c\u7b46\u70ba\u6e2c\u8a66\u96c6\u3002\u6240\u6709\u5716\u7247\u88ab\u5206\u70ba 10 \u500b\u985e\u5225\uff1a 0\uff1aairplane 1\uff1aautomobile 2\uff1abird 3\uff1acat 4\uff1adeer 5\uff1adog 6\uff1afrog 7\uff1ahorse 8\uff1aship 9\uff1atruck</p> <p>\u8acb\u5728 keras \u4e2d\u5229\u7528 DNN \u76e1\u4f60\u6240\u80fd\u7684\u8a13\u7df4\u6a21\u578b\uff0c\u4e26\u5831\u544a\u6700\u597d\u7684\u4e00\u6b21\u7d50\u679c\uff0c\u672c\u6b21\u4f5c\u696d\u7981\u6b62\u4f7f\u7528 pre-training model\uff0c\u8ab2\u5802\u4e0a\u672a\u63d0\u53ca\u7684\u65b9\u6cd5\u8acb\u9644\u52a0\u8aaa\u660e\u3002 <pre><code>import Keras\nfrom Keras.datasets import cifar10\n</code></pre></p>"},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/","title":"CIFAR-10","text":"In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom keras.datasets import cifar10\nfrom keras import models\nfrom keras import layers\n\n# Load CIFAR-10 dataset\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# Normalize pixel values between 0 and 1\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n</pre> import tensorflow as tf from keras.datasets import cifar10 from keras import models from keras import layers  # Load CIFAR-10 dataset (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()  # Normalize pixel values between 0 and 1 train_images, test_images = train_images / 255.0, test_images / 255.0 In\u00a0[\u00a0]: Copied! <pre>def build_model():\n    \n    model = models.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(10)\n    ])\n\n    model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n    \n    return model\n</pre> def build_model():          model = models.Sequential([         layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),         layers.MaxPooling2D((2, 2)),         layers.Conv2D(64, (3, 3), activation='relu'),         layers.MaxPooling2D((2, 2)),         layers.Conv2D(64, (3, 3), activation='relu'),         layers.Flatten(),         layers.Dense(64, activation='relu'),         layers.Dense(10)     ])      model.compile(optimizer='adam',               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=['accuracy'])          return model In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt # pip install matplotlib\nfrom random import randrange\n\ntext = ['airplane', \n        'automobile',\n        'bird' ,\n        'cat', \n        'deer', \n        'dog', \n        'frog', \n        'horse', \n        'ship', \n        'truck']\nplt.figure(figsize=(16,10),facecolor='w')\nfor i in range(5):\n  for j in range(8):\n    index = randrange(0, 50000)\n    plt.subplot(5, 8, i * 8 + j + 1)\n    plt.title(\"label: {}\".format(text[train_labels[index][0]]))\n    plt.imshow(train_images[index])\n    plt.axis('off')\n\nplt.show()\n</pre> import matplotlib.pyplot as plt # pip install matplotlib from random import randrange  text = ['airplane',          'automobile',         'bird' ,         'cat',          'deer',          'dog',          'frog',          'horse',          'ship',          'truck'] plt.figure(figsize=(16,10),facecolor='w') for i in range(5):   for j in range(8):     index = randrange(0, 50000)     plt.subplot(5, 8, i * 8 + j + 1)     plt.title(\"label: {}\".format(text[train_labels[index][0]]))     plt.imshow(train_images[index])     plt.axis('off')  plt.show() In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=20, \n    validation_data=(test_images, test_labels)\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=20,      validation_data=(test_images, test_labels) ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>history_dict = history.history\nhistory_dict.keys()\n</pre> history_dict = history.history history_dict.keys() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=7, \n    validation_data=(test_images, test_labels),\n    batch_size=800\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=7,      validation_data=(test_images, test_labels),     batch_size=800 ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>history_dict = history.history\nhistory_dict.keys()\n</pre> history_dict = history.history history_dict.keys() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>def build_model():\n    \n    model = models.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(10)\n    ])\n\n    model.compile(optimizer='rmsprop',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n    \n    return model\n</pre> def build_model():          model = models.Sequential([         layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),         layers.MaxPooling2D((2, 2)),         layers.Conv2D(64, (3, 3), activation='relu'),         layers.MaxPooling2D((2, 2)),         layers.Conv2D(64, (3, 3), activation='relu'),         layers.Flatten(),         layers.Dense(64, activation='relu'),         layers.Dense(10)     ])      model.compile(optimizer='rmsprop',               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=['accuracy'])          return model In\u00a0[\u00a0]: Copied! <pre>model = build_model()\n\n# Train the model\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=10, \n    validation_data=(test_images, test_labels),\n    batch_size=50\n)\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n</pre> model = build_model()  # Train the model history = model.fit(     train_images,      train_labels,      epochs=10,      validation_data=(test_images, test_labels),     batch_size=50 ) # Evaluate the model test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc) <p>Below are the experiments I have conducted:</p> <ul> <li>epoches: from 20 -&gt; 7.</li> <li>batch_size: from 100 -&gt; 50.</li> <li>optimizer: adam and rmsprop.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.datasets import cifar10\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import SGD, Adam\nfrom keras import models, layers\nimport keras as k\nfrom keras.optimizers import SGD, Adam\nfrom keras.regularizers import l2\nimport h5py\nfrom keras.models import load_model\nfrom keras.preprocessing.image import ImageDataGenerator\n</pre> import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from keras.datasets import cifar10 from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.applications import VGG16 from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization from keras.optimizers import SGD, Adam from keras import models, layers import keras as k from keras.optimizers import SGD, Adam from keras.regularizers import l2 import h5py from keras.models import load_model from keras.preprocessing.image import ImageDataGenerator In\u00a0[\u00a0]: Copied! <pre>(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nimg_rows, img_cols , channels= 32,32,3\nfor i in range(0,9):\n    plt.subplot(330 + 1 + i)\n    plt.imshow(x_train[i])\nplt.show()\n</pre> (x_train, y_train), (x_test, y_test) = cifar10.load_data() img_rows, img_cols , channels= 32,32,3 for i in range(0,9):     plt.subplot(330 + 1 + i)     plt.imshow(x_train[i]) plt.show() In\u00a0[\u00a0]: Copied! <pre># set up image augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=15,\n    horizontal_flip=True,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n    #zoom_range=0.3\n    )\ndatagen.fit(x_train)\n</pre> # set up image augmentation datagen = ImageDataGenerator(     rotation_range=15,     horizontal_flip=True,     width_shift_range=0.1,     height_shift_range=0.1     #zoom_range=0.3     ) datagen.fit(x_train) In\u00a0[\u00a0]: Copied! <pre># see example augmentation images\nfor X_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\n    for i in range(0, 9):\n        plt.subplot(330 + 1 + i)\n        plt.imshow(X_batch[i].astype(np.uint8))\n    plt.show()\n    break\n</pre> # see example augmentation images for X_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):     for i in range(0, 9):         plt.subplot(330 + 1 + i)         plt.imshow(X_batch[i].astype(np.uint8))     plt.show()     break In\u00a0[\u00a0]: Copied! <pre>#reshape into images\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\ninput_shape = (img_rows, img_cols, 1)\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n</pre> #reshape into images x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels) input_shape = (img_rows, img_cols, 1) print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') In\u00a0[\u00a0]: Copied! <pre>#convert integers to float; normalise and center the mean\nx_train=x_train.astype(\"float32\")  \nx_test=x_test.astype(\"float32\")\nmean=np.mean(x_train)\nstd=np.std(x_train)\nx_test=(x_test-mean)/std\nx_train=(x_train-mean)/std\n</pre> #convert integers to float; normalise and center the mean x_train=x_train.astype(\"float32\")   x_test=x_test.astype(\"float32\") mean=np.mean(x_train) std=np.std(x_train) x_test=(x_test-mean)/std x_train=(x_train-mean)/std In\u00a0[\u00a0]: Copied! <pre># labels\nnum_classes=10\ny_train = k.utils.to_categorical(y_train, num_classes)\ny_test = k.utils.to_categorical(y_test, num_classes)\n</pre> # labels num_classes=10 y_train = k.utils.to_categorical(y_train, num_classes) y_test = k.utils.to_categorical(y_test, num_classes) In\u00a0[\u00a0]: Copied! <pre># plotting helper function\ndef plothist(hist):\n    plt.plot(hist.history['accuracy'])\n    plt.plot(hist.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n</pre> # plotting helper function def plothist(hist):     plt.plot(hist.history['accuracy'])     plt.plot(hist.history['val_accuracy'])     plt.title('model accuracy')     plt.ylabel('accuracy')     plt.xlabel('epoch')     plt.legend(['train', 'test'], loc='upper left')     plt.show() In\u00a0[\u00a0]: Copied! <pre># build and compile the model  (roughly following the VGG paper)\n\n#reg=l2(1e-4)   # L2 or \"ridge\" regularisation\nreg=None\nnum_filters=32\nac='relu'\nadm=Adam(learning_rate=0.001,decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nopt=adm\ndrop_dense=0.5\ndrop_conv=0\n\nmodel = Sequential()\n\nmodel.add(Conv2D(num_filters, (3, 3), activation=ac, kernel_regularizer=reg, input_shape=(img_rows, img_cols, channels),padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Conv2D(num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 16x16x3xnum_filters\nmodel.add(Dropout(drop_conv))\n\nmodel.add(Conv2D(2*num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Conv2D(2*num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 8x8x3x(2*num_filters)\nmodel.add(Dropout(drop_conv))\n\nmodel.add(Conv2D(4*num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Conv2D(4*num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 4x4x3x(4*num_filters)\nmodel.add(Dropout(drop_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation=ac,kernel_regularizer=reg))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(drop_dense))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)\n</pre> # build and compile the model  (roughly following the VGG paper)  #reg=l2(1e-4)   # L2 or \"ridge\" regularisation reg=None num_filters=32 ac='relu' adm=Adam(learning_rate=0.001,decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08) opt=adm drop_dense=0.5 drop_conv=0  model = Sequential()  model.add(Conv2D(num_filters, (3, 3), activation=ac, kernel_regularizer=reg, input_shape=(img_rows, img_cols, channels),padding='same')) model.add(BatchNormalization(axis=-1)) model.add(Conv2D(num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same')) model.add(BatchNormalization(axis=-1)) model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 16x16x3xnum_filters model.add(Dropout(drop_conv))  model.add(Conv2D(2*num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same')) model.add(BatchNormalization(axis=-1)) model.add(Conv2D(2*num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same')) model.add(BatchNormalization(axis=-1)) model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 8x8x3x(2*num_filters) model.add(Dropout(drop_conv))  model.add(Conv2D(4*num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same')) model.add(BatchNormalization(axis=-1)) model.add(Conv2D(4*num_filters, (3, 3), activation=ac,kernel_regularizer=reg,padding='same')) model.add(BatchNormalization(axis=-1)) model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 4x4x3x(4*num_filters) model.add(Dropout(drop_conv))  model.add(Flatten()) model.add(Dense(512, activation=ac,kernel_regularizer=reg)) model.add(BatchNormalization()) model.add(Dropout(drop_dense)) model.add(Dense(num_classes, activation='softmax'))  model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt) In\u00a0[\u00a0]: Copied! <pre>model.summary()\n</pre> model.summary() In\u00a0[\u00a0]: Copied! <pre># training without augmentation\nhistory=model.fit(x_train, y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test))\n</pre> # training without augmentation history=model.fit(x_train, y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test)) In\u00a0[\u00a0]: Copied! <pre>#training accuracy without dropout\ntrain_acc=model.evaluate(x_train,y_train,batch_size=128)\ntrain_acc\n# print('Test accuracy:', test_acc)\n</pre> #training accuracy without dropout train_acc=model.evaluate(x_train,y_train,batch_size=128) train_acc # print('Test accuracy:', test_acc) In\u00a0[\u00a0]: Copied! <pre>plothist(history)\n</pre> plothist(history) In\u00a0[\u00a0]: Copied! <pre># Evaluate the model\ntest_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n</pre> # Evaluate the model test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2) In\u00a0[\u00a0]: Copied! <pre>print('Test accuracy:', test_acc)\n</pre> print('Test accuracy:', test_acc)"},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/#cifar-10","title":"CIFAR-10\u00b6","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/#course-ap4064","title":"Course: AP4064\u00b6","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/#assignment-2","title":"Assignment: 2\u00b6","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/#major-atm","title":"Major: ATM\u00b6","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/#name-hugo-chunho-lin","title":"Name: Hugo ChunHo Lin\u00b6","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/#student-id-109601003","title":"Student Id: 109601003\u00b6","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/#import-the-package-and-the-dataset","title":"Import the package and the dataset\u00b6","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/#use-pre-trained-model-vgg16-and-data-augmentation","title":"Use pre-trained model VGG16 and data Augmentation\u00b6","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_cifar10_data_aug/#reference","title":"Reference\u00b6","text":"<p>Image-Augmentation-in-Keras-CIFAR-10-</p>"},{"location":"archive/2023_spring/hw02/23_spring_hw02_description/","title":"Description","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_description/#spring-2023","title":"Spring, 2023","text":""},{"location":"archive/2023_spring/hw02/23_spring_hw02_description/#hw02","title":"HW02","text":"<p>Please print all the interactive output without resorting to print, not only the last result. To do this, you should add the following code in Jupyter cell to the beginning of the file.</p> <p>1. Cifar10 \u8cc7\u6599\u5373\u5305\u542b 6 \u842c\u7b46 32*32 \u4f4e\u89e3\u6790\u5ea6\u4e4b\u5f69\u8272\u5716\u7247\uff0c\u5176\u4e2d 5 \u842c\u7b46\u70ba\u8a13\u7df4\u96c6\uff1b1 \u842c\u7b46\u70ba\u6e2c\u8a66\u96c6\u3002\u6240\u6709\u5716\u7247\u88ab\u5206\u70ba 10 \u500b\u985e\u5225\uff1a<code>0\uff1aairplane 1\uff1aautomobile 2\uff1abird 3\uff1acat 4\uff1adeer 5\uff1adog 6\uff1afrog 7\uff1ahorse 8\uff1aship 9\uff1atruck</code> \u8acb\u5728 Keras \u4e2d\u5229\u7528 CNN \u76e1\u4f60\u6240\u80fd\u7684\u8a13\u7df4\u6a21\u578b\u4e26\u5b8c\u6574\u5831\u544a\u51fa\u6700\u597d\u7684\u4e00\u6b21\u7d50\u679c\u53ca\u8207\u7b2c\u4e00\u6b21\u4f5c\u696d\u60a8\u8a13\u7df4\u51fa\u4f86\u7684 DNN \u6a21\u578b\u505a\u6bd4\u8f03\u3002\u7981\u6b62\u4f7f\u7528 pre-trained model \u8ab2\u5802\u4e0a\u672a\u63d0\u53ca\u7684\u65b9\u6cd5\u8acb\u9644\u52a0\u8aaa\u660e</p> <p>2. \u627f\u984c 1\uff0c\u4f7f\u7528 pre-trained model <code>VGG16</code> \u4f5c\u70ba\u6a21\u578b\uff0c\u4e26\u642d\u914d data augmentation(\u53effine-tuning)\uff0c\u5c07\u9810\u6e2c\u7d50\u679c\u8207\u984c 1 \u505a\u6bd4\u8f03\u3002 \u4f7f\u7528\uff1a <pre><code>VGG16(weights='imagenet', include_top=False, input_shape=(64,64,3))\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport Keras\nfrom Keras.datasets import cifar10\n</code></pre></p>"},{"location":"archive/2023_spring/intro/23_spring_hw_intro/","title":"Machine Learning at ECMWF: A roadmap for the next 10 years","text":"<p>\u95b1\u8b80\u6587\u7ae0\u5f8c\uff0c\u6574\u7406\u6210\u81f3\u5c11\u4e00\u9801 A4 \u7684\u6982\u8ff0\uff0c\u4e86\u89e3\u6b50\u6d32\u4e2d\u671f\u5929\u6c23\u9810\u5831\u4e2d\u5fc3\u5728\u6a5f\u5668\u5b78\u7fd2\u61c9\u7528\u81f3\u6c23\u8c61\u9818\u57df\u4e0a\u7684\u65b9\u5411</p>"},{"location":"deep_learning/ch02/01_tensors_intro/","title":"What is a tensor?","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nx = np.array(12)\nx\n</pre> import numpy as np  x = np.array(12) x In\u00a0[\u00a0]: Copied! <pre>x.ndim\n</pre> x.ndim In\u00a0[\u00a0]: Copied! <pre>x = np.array([12, 3, 6, 4])\nx\n</pre> x = np.array([12, 3, 6, 4]) x In\u00a0[\u00a0]: Copied! <pre>x.ndim\n</pre> x.ndim In\u00a0[\u00a0]: Copied! <pre>x = np.array([[5, 78, 2, 34, 0], \n              [6, 79, 3, 35, 1],\n              [7, 80, 4, 36, 2]]\n)\nx\n</pre> x = np.array([[5, 78, 2, 34, 0],                [6, 79, 3, 35, 1],               [7, 80, 4, 36, 2]] ) x In\u00a0[\u00a0]: Copied! <pre>x.ndim\n</pre> x.ndim In\u00a0[\u00a0]: Copied! <pre>x = np.array([[[5, 78, 2, 34, 0], \n               [6, 79, 3, 35, 1],\n               [7, 80, 4, 36, 2]],\n   \n             [[5, 78, 2, 34, 0],\n              [6, 79, 3, 35, 1], \n              [7, 80, 4, 36, 2]], \n   \n             [[5, 78, 2, 34, 0], \n              [6, 79, 3, 35, 1],\n              [7, 80, 4, 36, 2]]]\n)\nx\n# x.shape (3, 3, 5)\n</pre> x = np.array([[[5, 78, 2, 34, 0],                 [6, 79, 3, 35, 1],                [7, 80, 4, 36, 2]],                  [[5, 78, 2, 34, 0],               [6, 79, 3, 35, 1],                [7, 80, 4, 36, 2]],                   [[5, 78, 2, 34, 0],                [6, 79, 3, 35, 1],               [7, 80, 4, 36, 2]]] ) x # x.shape (3, 3, 5) In\u00a0[\u00a0]: Copied! <pre>x.ndim\n</pre> x.ndim <p>A tensor is defined by three key attributes:</p> <ul> <li>Number of axes (rank)\u2014For instance, a 3D tensor has three axes, and a matrix has two axes. This is also called the tensor\u2019s ndim in Python libraries such as Numpy.</li> <li>Shape\u2014This is a tuple of integers that describes how many dimensions the tensor has along each axis. For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5). A vector has a shape with a single element, such as (5,), whereas a scalar has an empty shape, ().</li> <li>Data type (usually called dtype in Python libraries)\u2014This is the type of the data contained in the tensor; for instance, a tensor\u2019s type could be float32, uint8, float64, and so on.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from keras.datasets import mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n</pre> from keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() In\u00a0[\u00a0]: Copied! <pre>print(train_images.ndim)\nprint(train_images.shape)\nprint(train_images.dtype)\n</pre> print(train_images.ndim) print(train_images.shape) print(train_images.dtype) In\u00a0[\u00a0]: Copied! <pre>digit = train_images[4]\n\nimport matplotlib.pyplot as plt\nplt.imshow(digit, cmap=plt.cm.binary)\nplt.show()\n</pre> digit = train_images[4]  import matplotlib.pyplot as plt plt.imshow(digit, cmap=plt.cm.binary) plt.show() In\u00a0[\u00a0]: Copied! <pre># pick up the digits 10 to 99 -&gt; include 90 digits.\nmy_slice = train_images[10: 100]\nprint(my_slice.shape)\n</pre> # pick up the digits 10 to 99 -&gt; include 90 digits. my_slice = train_images[10: 100] print(my_slice.shape) In\u00a0[\u00a0]: Copied! <pre>my_slice = train_images[10:100, :, :]\nprint(my_slice.shape)\n</pre> my_slice = train_images[10:100, :, :] print(my_slice.shape) In\u00a0[\u00a0]: Copied! <pre>my_slice = train_images[10:100, 0:28, 0:28]\nprint(my_slice.shape)\n</pre> my_slice = train_images[10:100, 0:28, 0:28] print(my_slice.shape) In\u00a0[\u00a0]: Copied! <pre># bottom-right\nmy_slice = train_images[:, 14:, 14:]\n# center\nmy_slice = train_images[:, 7:-7, 7:-7]\n</pre> # bottom-right my_slice = train_images[:, 14:, 14:] # center my_slice = train_images[:, 7:-7, 7:-7] In\u00a0[\u00a0]: Copied! <pre>batch = train_images[:128]\n# the next batch\nbatch = train_images[128:256]\n</pre> batch = train_images[:128] # the next batch batch = train_images[128:256]"},{"location":"deep_learning/ch02/01_tensors_intro/#what-is-a-tensor","title":"What is a tensor?\u00b6","text":""},{"location":"deep_learning/ch02/01_tensors_intro/#0-d-tensors","title":"0-D tensors\u00b6","text":""},{"location":"deep_learning/ch02/01_tensors_intro/#1-d-tensors","title":"1-D tensors.\u00b6","text":""},{"location":"deep_learning/ch02/01_tensors_intro/#2-d-tensors","title":"2-D tensors.\u00b6","text":""},{"location":"deep_learning/ch02/01_tensors_intro/#3-d-tensors","title":"3-D tensors.\u00b6","text":""},{"location":"deep_learning/ch02/01_tensors_intro/#display-the-forth-digit-in-this-3-d-tensor","title":"Display the forth digit in this 3-D tensor.\u00b6","text":""},{"location":"deep_learning/ch02/01_tensors_intro/#manipulating-tensor","title":"Manipulating tensor.\u00b6","text":""},{"location":"deep_learning/ch02/01_tensors_intro/#slice-the-feature-data","title":"Slice the feature data.\u00b6","text":""},{"location":"deep_learning/ch02/01_tensors_intro/#we-dont-process-an-entire-dataset-at-once-rather-we-will-break-the-data-into-small-batches","title":"We don't process an entire dataset at once; rather, we will break the data into small batches.\u00b6","text":""},{"location":"deep_learning/ch02/01_tensors_intro/#the-real-world-example-data","title":"The real-world example data.\u00b6","text":"<ul> <li>Vector data\u20142D tensors of shape (samples, features)</li> <li>Timeseries data or sequence data\u20143D tensors of shape (samples, timesteps, features)</li> <li>Images\u20144D tensors of shape (samples, height, width, channels) or (samples, channels, height, width)</li> <li>Video\u20145Dtensorsofshape(samples,frames,height,width, channels) or (samples, frames, channels, height, width)</li> </ul>"},{"location":"deep_learning/ch02/02_tensor_operation/","title":"The gear of neural network.","text":"In\u00a0[\u00a0]: Copied! <pre>from keras.datasets import mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n</pre> from keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() In\u00a0[\u00a0]: Copied! <pre>import keras\nfrom keras import models\nfrom keras import layers\n</pre> import keras from keras import models from keras import layers In\u00a0[\u00a0]: Copied! <pre>keras.layers.Dense(512, activation='relu')\n</pre> keras.layers.Dense(512, activation='relu') In\u00a0[\u00a0]: Copied! <pre># output = relu(dot(W, input) + b)\n</pre> # output = relu(dot(W, input) + b) In\u00a0[\u00a0]: Copied! <pre>def naive_relu(x):\n\"\"\"_keep the number greater than zero._\n\n    Args:\n        x (_type_): _description_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    assert len(x.shape) == 2\n    x = x.copy()\n    \n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n          x[i, j] = max(x[i, j], 0)\n\n    return x\n</pre> def naive_relu(x):     \"\"\"_keep the number greater than zero._      Args:         x (_type_): _description_      Returns:         _type_: _description_     \"\"\"     assert len(x.shape) == 2     x = x.copy()          for i in range(x.shape[0]):         for j in range(x.shape[1]):           x[i, j] = max(x[i, j], 0)      return x In\u00a0[\u00a0]: Copied! <pre>def naive_add(x, y):\n\n\"\"\"_x+y_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    \n    assert len(x.shape) == 2 \n    assert x.shape == y.shape\n    x = x.copy()\n\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]): x[i, j] += y[i, j]\n\n    return x\n</pre> def naive_add(x, y):      \"\"\"_x+y_      Returns:         _type_: _description_     \"\"\"          assert len(x.shape) == 2      assert x.shape == y.shape     x = x.copy()      for i in range(x.shape[0]):         for j in range(x.shape[1]): x[i, j] += y[i, j]      return x In\u00a0[\u00a0]: Copied! <pre>import numpy as np \n\nx = np.random.random((64,  3)) \ny = np.random.random((32, 10))\nz = x + y\nz = np.maximum(z, 0.)\n</pre> import numpy as np   x = np.random.random((64,  3))  y = np.random.random((32, 10)) z = x + y z = np.maximum(z, 0.) In\u00a0[\u00a0]: Copied! <pre>def naive_add_matrix_and_vector(x, y): \n    \n    assert len(x.shape) == 2\n    assert len(y.shape) == 1\n    assert x.shape[1] == y.shape[0]\n    x = x.copy()\n    \n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]): x[i, j] += y[j]\n    \n    return x\n</pre> def naive_add_matrix_and_vector(x, y):           assert len(x.shape) == 2     assert len(y.shape) == 1     assert x.shape[1] == y.shape[0]     x = x.copy()          for i in range(x.shape[0]):         for j in range(x.shape[1]): x[i, j] += y[j]          return x In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nx = np.random.random((64,  3, 32, 10)) \ny = np.random.random((32, 10))\nz = np.maximum(x, y)\n</pre> import numpy as np  x = np.random.random((64,  3, 32, 10))  y = np.random.random((32, 10)) z = np.maximum(x, y) In\u00a0[\u00a0]: Copied! <pre>import numpy as np \n\nz = np.dot(x, y)\n</pre> import numpy as np   z = np.dot(x, y) In\u00a0[\u00a0]: Copied! <pre>def naive_vector_dot(x, y): \n\n    assert len(x.shape) == 1\n    assert len(y.shape) == 1\n    assert x.shape[0] == y.shape[0] \n    z = 0.\n    \n    for i in range(x.shape[0]): z += x[i] * y[i]\n\n    return z\n</pre> def naive_vector_dot(x, y):       assert len(x.shape) == 1     assert len(y.shape) == 1     assert x.shape[0] == y.shape[0]      z = 0.          for i in range(x.shape[0]): z += x[i] * y[i]      return z In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\ndef naive_matrix_vector_dot(x, y):\n\n    assert len(x.shape) == 2\n    assert len(y.shape) == 1\n    assert x.shape[1] == y.shape[0] \n    z = np.zeros(x.shape[0])\n    \n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            z[i] += x[i, j] * y[j]\n\n    return z\n</pre> import numpy as np  def naive_matrix_vector_dot(x, y):      assert len(x.shape) == 2     assert len(y.shape) == 1     assert x.shape[1] == y.shape[0]      z = np.zeros(x.shape[0])          for i in range(x.shape[0]):         for j in range(x.shape[1]):             z[i] += x[i, j] * y[j]      return z In\u00a0[\u00a0]: Copied! <pre>def naive_matrix_vector_dot(x, y):\n    \n    z = np.zeros(x.shape[0])\n\n    for i in range(x.shape[0]):\n        z[i] = naive_vector_dot(x[i, :], y) \n    \n    return z\n</pre> def naive_matrix_vector_dot(x, y):          z = np.zeros(x.shape[0])      for i in range(x.shape[0]):         z[i] = naive_vector_dot(x[i, :], y)           return z In\u00a0[\u00a0]: Copied! <pre>def naive_matrix_dot(x, y):\n    \n    assert len(x.shape) == 2\n    assert len(y.shape) == 2\n    assert x.shape[1] == y.shape[0]\n    z = np.zeros((x.shape[0], y.shape[1])) \n    \n    for i in range(x.shape[0]):\n        for j in range(y.shape[1]):\n            row_x = x[i, :]\n            column_y = y[:, j]\n            z[i, j] = naive_vector_dot(row_x, column_y)\n    return z\n</pre> def naive_matrix_dot(x, y):          assert len(x.shape) == 2     assert len(y.shape) == 2     assert x.shape[1] == y.shape[0]     z = np.zeros((x.shape[0], y.shape[1]))           for i in range(x.shape[0]):         for j in range(y.shape[1]):             row_x = x[i, :]             column_y = y[:, j]             z[i, j] = naive_vector_dot(row_x, column_y)     return z In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nx = np.array([[0., 1.], \n              [2., 3.],\n              [4., 5.]])\nprint(x)\n</pre> import numpy as np  x = np.array([[0., 1.],                [2., 3.],               [4., 5.]]) print(x) In\u00a0[\u00a0]: Copied! <pre>x = x.reshape((6, 1))\nx\n</pre> x = x.reshape((6, 1)) x In\u00a0[\u00a0]: Copied! <pre>x = x.reshape((2, 3))\nx\n</pre> x = x.reshape((2, 3)) x In\u00a0[\u00a0]: Copied! <pre>x = np.zeros((300, 20))\nx = np.transpose(x)\n\nprint(x.shape)\n</pre> x = np.zeros((300, 20)) x = np.transpose(x)  print(x.shape) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"deep_learning/ch02/02_tensor_operation/#the-gear-of-neural-network","title":"The gear of neural network.\u00b6","text":""},{"location":"deep_learning/ch02/02_tensor_operation/#broadcasting","title":"Broadcasting\u00b6","text":"<p>let small tensors match the larger tensor.</p>"},{"location":"deep_learning/ch02/02_tensor_operation/#dot","title":"Dot.\u00b6","text":""},{"location":"deep_learning/ch02/02_tensor_operation/#reshaping","title":"Reshaping\u00b6","text":""},{"location":"deep_learning/ch02/03_handwriting/","title":"MNIST Handwriting Number Detection","text":"<p>The first look at a neural network</p> In\u00a0[\u00a0]: Copied! <pre>from keras.datasets import mnist\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n</pre> from keras.datasets import mnist  (train_images, train_labels), (test_images, test_labels) = mnist.load_data() <p>Observe the train data.</p> In\u00a0[\u00a0]: Copied! <pre>train_images.shape\n</pre> train_images.shape In\u00a0[\u00a0]: Copied! <pre>len(train_labels)\n</pre> len(train_labels) In\u00a0[\u00a0]: Copied! <pre>train_labels\n</pre> train_labels <p>Observe the test data.</p> In\u00a0[\u00a0]: Copied! <pre>test_images.shape\n</pre> test_images.shape In\u00a0[\u00a0]: Copied! <pre>len(test_labels)\n</pre> len(test_labels) In\u00a0[\u00a0]: Copied! <pre>test_labels\n</pre> test_labels <p>Build the neural network models.</p> In\u00a0[\u00a0]: Copied! <pre>from keras import models\nfrom keras import layers\n\nnetwork = models.Sequential()\n# after we build the sequential, we add the layer.\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n# the sum of ten number is 10.\nnetwork.add(layers.Dense(10, activation='softmax'))\n</pre> from keras import models from keras import layers  network = models.Sequential() # after we build the sequential, we add the layer. network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,))) # the sum of ten number is 10. network.add(layers.Dense(10, activation='softmax')) <p>The compliation step</p> In\u00a0[\u00a0]: Copied! <pre>network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n</pre> network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) <p>Prepare the image data to training and testing</p> In\u00a0[\u00a0]: Copied! <pre>train_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype('float32') / 255\ntest_images = test_images.reshape((10000, 28 * 28))\ntest_images = test_images.astype('float32') / 255\n</pre> train_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype('float32') / 255 test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype('float32') / 255 <p>Label the traing data and testing data</p> In\u00a0[\u00a0]: Copied! <pre>from keras.utils import to_categorical\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n</pre> from keras.utils import to_categorical train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) In\u00a0[\u00a0]: Copied! <pre># like the leaf classifier turn the number into percentage.\ntrain_labels[0]\n</pre> # like the leaf classifier turn the number into percentage. train_labels[0] <p>We are ready to train the network with <code>fit</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>network.fit(train_images, train_labels, epochs=5, batch_size=128)\n</pre> network.fit(train_images, train_labels, epochs=5, batch_size=128) <p>We have reached an accuracy of 0.989% on training data. Then we check the model performs well on the test set.</p> In\u00a0[\u00a0]: Copied! <pre>test_loss, test_acc = network.evaluate(test_images, test_labels)\n\nprint('test_acc:', test_acc)\n</pre> test_loss, test_acc = network.evaluate(test_images, test_labels)  print('test_acc:', test_acc) <p>We got the quite a bit lower than the training data set -&gt; overfitting. Machine-learning models tend to perform worse on new data.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"deep_learning/ch02/03_handwriting/#mnist-handwriting-number-detection","title":"MNIST Handwriting Number Detection\u00b6","text":""},{"location":"deep_learning/ch03/01_binary_classification/","title":"Two-class Classification","text":"In\u00a0[\u00a0]: Copied! <pre>from keras.datasets import imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n</pre> from keras.datasets import imdb  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) In\u00a0[\u00a0]: Copied! <pre>print(train_data[0])\nprint(train_labels[0])\n</pre> print(train_data[0]) print(train_labels[0]) In\u00a0[\u00a0]: Copied! <pre># max([max(sequence) for sequence in train_data])\n</pre> # max([max(sequence) for sequence in train_data]) In\u00a0[\u00a0]: Copied! <pre>word_index = imdb.get_word_index()\nreverse_word_index = dict(\n    [(value, key) for (key, value) in word_index.items()]\n)\ndecoded_review = ' '.join(\n    [reverse_word_index.get(i - 3, '?') for i in train_data[0]]\n)\n</pre> word_index = imdb.get_word_index() reverse_word_index = dict(     [(value, key) for (key, value) in word_index.items()] ) decoded_review = ' '.join(     [reverse_word_index.get(i - 3, '?') for i in train_data[0]] ) In\u00a0[\u00a0]: Copied! <pre># word_index.items()\n</pre> # word_index.items() In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension)) \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1. \n    return results\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n</pre> import numpy as np  def vectorize_sequences(sequences, dimension=10000):     results = np.zeros((len(sequences), dimension))      for i, sequence in enumerate(sequences):         results[i, sequence] = 1.      return results  x_train = vectorize_sequences(train_data) x_test = vectorize_sequences(test_data) In\u00a0[\u00a0]: Copied! <pre>y_train = np.asarray(train_labels).astype('float32') \ny_test = np.asarray(test_labels).astype('float32')\n</pre> y_train = np.asarray(train_labels).astype('float32')  y_test = np.asarray(test_labels).astype('float32') In\u00a0[\u00a0]: Copied! <pre>from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \nmodel.add(layers.Dense(16, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid'))\n</pre> from keras import models from keras import layers  model = models.Sequential() model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))  model.add(layers.Dense(16, activation='relu'))  model.add(layers.Dense(1, activation='sigmoid')) In\u00a0[\u00a0]: Copied! <pre>model.compile(\n    optimizer='rmsprop', \n    loss='binary_crossentropy', \n    metrics=['accuracy'],\n)\n</pre> model.compile(     optimizer='rmsprop',      loss='binary_crossentropy',      metrics=['accuracy'], ) In\u00a0[\u00a0]: Copied! <pre>from keras import optimizers \n\nmodel.compile(\n    optimizer=optimizers.RMSprop(lr=0.001), \n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n</pre> from keras import optimizers   model.compile(     optimizer=optimizers.RMSprop(lr=0.001),      loss='binary_crossentropy',     metrics=['accuracy'] ) In\u00a0[\u00a0]: Copied! <pre>from keras import losses\nfrom keras import metrics \n\nmodel.compile(\n    optimizer=optimizers.RMSprop(lr=0.001), \n    loss=losses.binary_crossentropy, \n    metrics=[metrics.binary_accuracy]\n)\n</pre> from keras import losses from keras import metrics   model.compile(     optimizer=optimizers.RMSprop(lr=0.001),      loss=losses.binary_crossentropy,      metrics=[metrics.binary_accuracy] ) In\u00a0[\u00a0]: Copied! <pre>x_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n</pre> x_val = x_train[:10000] partial_x_train = x_train[10000:] y_val = y_train[:10000] partial_y_train = y_train[10000:] In\u00a0[\u00a0]: Copied! <pre>model.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['acc']\n) \nhistory = model.fit(\n    partial_x_train, \n    partial_y_train,\n    epochs=20,\n    batch_size=512, \n    validation_data=(x_val, y_val)\n)\n</pre> model.compile(     optimizer='rmsprop',     loss='binary_crossentropy',     metrics=['acc'] )  history = model.fit(     partial_x_train,      partial_y_train,     epochs=20,     batch_size=512,      validation_data=(x_val, y_val) ) In\u00a0[\u00a0]: Copied! <pre>history_dict = history.history\nhistory_dict.keys()\n</pre> history_dict = history.history history_dict.keys() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss') \nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.xlabel('Epochs') \nplt.ylabel('Loss') \nplt.legend() \n\nplt.show()\n</pre> import matplotlib.pyplot as plt  history_dict = history.history loss_values = history_dict['loss'] val_loss_values = history_dict['val_loss']  epochs = range(1, len(loss_values) + 1) plt.plot(epochs, loss_values, 'bo', label='Training loss')  plt.plot(epochs, val_loss_values, 'b', label='Validation loss')  plt.title('Training and validation loss') plt.xlabel('Epochs')  plt.ylabel('Loss')  plt.legend()   plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.clf()\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc'] \nplt.plot(epochs, acc_values, 'bo', label='Training acc') \nplt.plot(epochs, val_acc_values, 'b', label='Validation acc') \nplt.title('Training and validation accuracy') \nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n</pre> plt.clf() acc_values = history_dict['acc'] val_acc_values = history_dict['val_acc']  plt.plot(epochs, acc_values, 'bo', label='Training acc')  plt.plot(epochs, val_acc_values, 'b', label='Validation acc')  plt.title('Training and validation accuracy')  plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \nmodel.add(layers.Dense(16, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid')) \nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\nmodel.fit(\n    x_train, \n    y_train, \n    epochs=4, \n    batch_size=512\n)\n</pre> model = models.Sequential() model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))  model.add(layers.Dense(16, activation='relu'))  model.add(layers.Dense(1, activation='sigmoid'))  model.compile(     optimizer='rmsprop',     loss='binary_crossentropy',     metrics=['accuracy'] ) model.fit(     x_train,      y_train,      epochs=4,      batch_size=512 ) In\u00a0[\u00a0]: Copied! <pre>results = model.evaluate(x_test, y_test)\nprint(results)\n</pre> results = model.evaluate(x_test, y_test) print(results) In\u00a0[\u00a0]: Copied! <pre>model.predict(x_test)\n</pre> model.predict(x_test) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"deep_learning/ch03/01_binary_classification/#two-class-classification","title":"Two-class Classification\u00b6","text":""},{"location":"deep_learning/ch03/01_binary_classification/#import-the-imdb-dataset","title":"Import the IMDB dataset\u00b6","text":""},{"location":"deep_learning/ch03/01_binary_classification/#preparing-the-data","title":"Preparing the data.\u00b6","text":""},{"location":"deep_learning/ch03/01_binary_classification/#building-the-network","title":"Building the Network\u00b6","text":"<p>relu: meant to zero out negative values</p> <p>sigmoid: \u201csquashes\u201d arbitrary values into the [0, 1] interval</p>"},{"location":"deep_learning/ch03/01_binary_classification/#three-methods-to-compile-the-model","title":"Three methods to compile the model.\u00b6","text":""},{"location":"deep_learning/ch03/01_binary_classification/#visualize-the-training-progress","title":"Visualize the training progress.\u00b6","text":""},{"location":"deep_learning/ch03/01_binary_classification/#retrain-a-model","title":"Retrain a model\u00b6","text":""},{"location":"deep_learning/ch03/02_multiclass_classification/","title":"Classifying newswires: a multiclass classification example","text":"In\u00a0[\u00a0]: Copied! <pre>from keras.datasets import reuters\n\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000)\n</pre> from keras.datasets import reuters  (train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000) In\u00a0[\u00a0]: Copied! <pre>print('length of training data:', len(train_data))\nprint('length of testing data:', len(test_data))\n</pre> print('length of training data:', len(train_data)) print('length of testing data:', len(test_data)) In\u00a0[\u00a0]: Copied! <pre>train_data[10]\n</pre> train_data[10] In\u00a0[\u00a0]: Copied! <pre>word_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) \ndecoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n</pre> word_index = reuters.get_word_index() reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])  decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]]) In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\ndef vectorize_sequences(sequences, dimension=10000):\n\n    results = np.zeros((len(sequences), dimension)) \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1. \n    \n    return results\n    \nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n</pre> import numpy as np  def vectorize_sequences(sequences, dimension=10000):      results = np.zeros((len(sequences), dimension))      for i, sequence in enumerate(sequences):         results[i, sequence] = 1.           return results      x_train = vectorize_sequences(train_data) x_test = vectorize_sequences(test_data) <p>One-hot encoding is a widely used format for categorical data, also called categorical encoding.</p> In\u00a0[\u00a0]: Copied! <pre>def to_one_hot(labels, dimension=46):\n    \n    results = np.zeros((len(labels), dimension)) \n    for i, label in enumerate(labels):\n        results[i, label] = 1. \n    return results\n\none_hot_train_labels = to_one_hot(train_labels) \none_hot_test_labels = to_one_hot(test_labels)\n</pre> def to_one_hot(labels, dimension=46):          results = np.zeros((len(labels), dimension))      for i, label in enumerate(labels):         results[i, label] = 1.      return results  one_hot_train_labels = to_one_hot(train_labels)  one_hot_test_labels = to_one_hot(test_labels) In\u00a0[\u00a0]: Copied! <pre>from keras.utils.np_utils import to_categorical \n\none_hot_train_labels = to_categorical(train_labels) \none_hot_test_labels = to_categorical(test_labels)\n</pre> from keras.utils.np_utils import to_categorical   one_hot_train_labels = to_categorical(train_labels)  one_hot_test_labels = to_categorical(test_labels) In\u00a0[\u00a0]: Copied! <pre>from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \nmodel.add(layers.Dense(64, activation='relu')) \nmodel.add(layers.Dense(46, activation='softmax'))\n</pre> from keras import models from keras import layers  model = models.Sequential() model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))  model.add(layers.Dense(64, activation='relu'))  model.add(layers.Dense(46, activation='softmax')) In\u00a0[\u00a0]: Copied! <pre>model.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n</pre> model.compile(     optimizer='rmsprop',     loss='categorical_crossentropy',     metrics=['accuracy'] ) In\u00a0[\u00a0]: Copied! <pre>x_val = x_train[:1000]\npartial_x_train = x_train[1000:]\ny_val = one_hot_train_labels[:1000] \npartial_y_train = one_hot_train_labels[1000:]\n</pre> x_val = x_train[:1000] partial_x_train = x_train[1000:] y_val = one_hot_train_labels[:1000]  partial_y_train = one_hot_train_labels[1000:] In\u00a0[\u00a0]: Copied! <pre>history = model.fit(\n    partial_x_train, \n    partial_y_train,\n    epochs=20,      \n    batch_size=512, \n    validation_data=(x_val, y_val)\n)\n</pre> history = model.fit(     partial_x_train,      partial_y_train,     epochs=20,           batch_size=512,      validation_data=(x_val, y_val) ) In\u00a0[\u00a0]: Copied! <pre>history_dict = history.history\nhistory_dict.keys()\n</pre> history_dict = history.history history_dict.keys() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss') \nplt.plot(epochs, val_loss, 'b', label='Validation loss') \nplt.title('Training and validation loss') \nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n</pre> import matplotlib.pyplot as plt  loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(1, len(loss) + 1) plt.plot(epochs, loss, 'bo', label='Training loss')  plt.plot(epochs, val_loss, 'b', label='Validation loss')  plt.title('Training and validation loss')  plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend()  plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.clf()\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplt.plot(epochs, acc, 'bo', label='Training acc') \nplt.plot(epochs, val_acc, 'b', label='Validation acc') \nplt.title('Training and validation accuracy') \nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n</pre> plt.clf() acc = history.history['accuracy'] val_acc = history.history['val_accuracy'] plt.plot(epochs, acc, 'bo', label='Training acc')  plt.plot(epochs, val_acc, 'b', label='Validation acc')  plt.title('Training and validation accuracy')  plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend()  plt.show() In\u00a0[\u00a0]: Copied! <pre>model = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \nmodel.add(layers.Dense(64, activation='relu')) \nmodel.add(layers.Dense(46, activation='softmax')) \nmodel.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.fit(\n    partial_x_train,\n    partial_y_train,\n    epochs=9,\n    batch_size=512,\n    validation_data=(x_val, y_val)\n)\n</pre> model = models.Sequential() model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))  model.add(layers.Dense(64, activation='relu'))  model.add(layers.Dense(46, activation='softmax'))  model.compile(     optimizer='rmsprop',     loss='categorical_crossentropy',     metrics=['accuracy'] ) model.fit(     partial_x_train,     partial_y_train,     epochs=9,     batch_size=512,     validation_data=(x_val, y_val) ) In\u00a0[\u00a0]: Copied! <pre>results = model.evaluate(x_test, one_hot_test_labels)\nprint(results)\n</pre> results = model.evaluate(x_test, one_hot_test_labels) print(results) In\u00a0[\u00a0]: Copied! <pre>import copy\n\ntest_labels_copy = copy.copy(test_labels)\nnp.random.shuffle(test_labels_copy)\nhits_array = np.array(test_labels) == np.array(test_labels_copy)\nfloat(np.sum(hits_array)) / len(test_labels)\n</pre> import copy  test_labels_copy = copy.copy(test_labels) np.random.shuffle(test_labels_copy) hits_array = np.array(test_labels) == np.array(test_labels_copy) float(np.sum(hits_array)) / len(test_labels) In\u00a0[\u00a0]: Copied! <pre>predictions = model.predict(x_test)\n</pre> predictions = model.predict(x_test) In\u00a0[\u00a0]: Copied! <pre>print('shape of predictions:', predictions[0].shape)\nprint('the coefficients in this vector sum:', np.sum(predictions[0]))\nprint('the class with the highest probability:', np.argmax(predictions[0]))\n</pre> print('shape of predictions:', predictions[0].shape) print('the coefficients in this vector sum:', np.sum(predictions[0])) print('the class with the highest probability:', np.argmax(predictions[0])) In\u00a0[\u00a0]: Copied! <pre>y_train = np.array(train_labels)\ny_test = np.array(test_labels)\n\nmodel.compile(\n    optimizer='rmsprop', \n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n)\n</pre> y_train = np.array(train_labels) y_test = np.array(test_labels)  model.compile(     optimizer='rmsprop',      loss='sparse_categorical_crossentropy',      metrics=['acc'] ) In\u00a0[\u00a0]: Copied! <pre>model = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \nmodel.add(layers.Dense(4, activation='relu')) \nmodel.add(layers.Dense(46, activation='softmax')) \nmodel.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n) \nmodel.fit(\n    partial_x_train,\n    partial_y_train,\n    epochs=20,\n    batch_size=128,\n    validation_data=(x_val, y_val)\n)\n</pre> model = models.Sequential() model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))  model.add(layers.Dense(4, activation='relu'))  model.add(layers.Dense(46, activation='softmax'))  model.compile(     optimizer='rmsprop',     loss='categorical_crossentropy',     metrics=['accuracy'] )  model.fit(     partial_x_train,     partial_y_train,     epochs=20,     batch_size=128,     validation_data=(x_val, y_val) ) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"deep_learning/ch03/02_multiclass_classification/#classifying-newswires-a-multiclass-classification-example","title":"Classifying newswires: a multiclass classification example\u00b6","text":""},{"location":"deep_learning/ch03/02_multiclass_classification/#building-the-network","title":"Building the network\u00b6","text":"<p>softmax: means the network will output a probability distribution over the 46 different output classes\u2014for every input sample</p>"},{"location":"deep_learning/ch03/02_multiclass_classification/#split-the-data","title":"Split the data.\u00b6","text":""},{"location":"deep_learning/ch03/02_multiclass_classification/#retrain-the-data","title":"Retrain the data.\u00b6","text":""},{"location":"deep_learning/ch03/02_multiclass_classification/#further-experiments","title":"Further experiments\u00b6","text":"<ul> <li>Try using larger or smaller layers: 32 units, 128 units, and so on.</li> <li>You used two hidden layers. Now try using a single hidden layer, or three hidden layers.</li> </ul>"},{"location":"deep_learning/ch03/03_regression/","title":"The Boston Housing Price dataset","text":"In\u00a0[\u00a0]: Copied! <pre>from keras.datasets import boston_housing\n\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n</pre> from keras.datasets import boston_housing  (train_data, train_targets), (test_data, test_targets) = boston_housing.load_data() In\u00a0[\u00a0]: Copied! <pre>print('shape of train data:', train_data.shape)\nprint('shape of test data:', test_data.shape)\nprint('train targets:', train_targets)\n</pre> print('shape of train data:', train_data.shape) print('shape of test data:', test_data.shape) print('train targets:', train_targets) <p>Average: 0, standard: 1</p> In\u00a0[\u00a0]: Copied! <pre>mean = train_data.mean(axis=0) \nstd = train_data.std(axis=0) \n\ntrain_data -= mean\ntrain_data /= std\n\n\"\"\"\nWe should never use in our workflow any \nquantity computed on the test data, even\nfor something as simple as data normalization.\n\"\"\"\n# test_data -= mean \n# test_data /= std\n</pre> mean = train_data.mean(axis=0)  std = train_data.std(axis=0)   train_data -= mean train_data /= std  \"\"\" We should never use in our workflow any  quantity computed on the test data, even for something as simple as data normalization. \"\"\" # test_data -= mean  # test_data /= std In\u00a0[\u00a0]: Copied! <pre>from keras import models\nfrom keras import layers\n\ndef build_model():\n    \n    model = models.Sequential()\n    model.add(layers.Dense(\n        64, \n        activation='relu', \n        input_shape=(train_data.shape[1],))\n    ) \n    model.add(layers.Dense(\n        64, \n        activation='relu')\n    ) \n    model.add(layers.Dense(1))\n    model.compile(\n        optimizer='rmsprop', \n        loss='mse', \n        metrics=['mae']\n    )\n    \n    return model\n</pre> from keras import models from keras import layers  def build_model():          model = models.Sequential()     model.add(layers.Dense(         64,          activation='relu',          input_shape=(train_data.shape[1],))     )      model.add(layers.Dense(         64,          activation='relu')     )      model.add(layers.Dense(1))     model.compile(         optimizer='rmsprop',          loss='mse',          metrics=['mae']     )          return model In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nk = 4\nnum_val_samples = len(train_data) // k \nnum_epochs = 100\nall_scores = []\n\nfor i in range(k):\n\n    print('processing fold #', i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] \n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] \n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples], \n        train_data[(i + 1) * num_val_samples:]], \n        axis=0\n    )\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples], \n        train_targets[(i + 1) * num_val_samples:]], \n        axis=0\n    )\n    model = build_model()\n    model.fit(\n        partial_train_data,\n        partial_train_targets,\n        epochs=num_epochs,\n        batch_size=1,\n        verbose=0,\n        # verbose=0,\n    )\n    val_mse, val_mae = model.evaluate(\n        val_data,\n        val_targets,\n        verbose=0,\n        # verbose=0,\n    )\n    all_scores.append(val_mae)\n</pre> import numpy as np  k = 4 num_val_samples = len(train_data) // k  num_epochs = 100 all_scores = []  for i in range(k):      print('processing fold #', i)     val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]      val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]      partial_train_data = np.concatenate(         [train_data[:i * num_val_samples],          train_data[(i + 1) * num_val_samples:]],          axis=0     )     partial_train_targets = np.concatenate(         [train_targets[:i * num_val_samples],          train_targets[(i + 1) * num_val_samples:]],          axis=0     )     model = build_model()     model.fit(         partial_train_data,         partial_train_targets,         epochs=num_epochs,         batch_size=1,         verbose=0,         # verbose=0,     )     val_mse, val_mae = model.evaluate(         val_data,         val_targets,         verbose=0,         # verbose=0,     )     all_scores.append(val_mae) In\u00a0[\u00a0]: Copied! <pre>all_scores\n</pre> all_scores In\u00a0[\u00a0]: Copied! <pre>np.mean(all_scores)\n</pre> np.mean(all_scores) In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nk = 4\nnum_val_samples = len(train_data) // k \nnum_epochs = 500 \nall_mae_histories = [] \n\nfor i in range(k):\n    print('processing fold #', i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] \n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] \n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples], \n        train_data[(i + 1) * num_val_samples:]], \n        axis=0\n    )\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples], \n        train_targets[(i + 1) * num_val_samples:]], \n        axis=0\n    )\n    model = build_model()\n    history = model.fit(\n        partial_train_data, \n        partial_train_targets,\n        validation_data=(val_data, val_targets),\n        epochs=num_epochs, \n        batch_size=1, \n        verbose=0\n    )\n    # mae_history = history.history['val_mean_absolute_error'] \n    mae_history = history.history['val_mae']\n    all_mae_histories.append(mae_history)\n    print('Training sucessfully in #', i)\n</pre> import numpy as np  k = 4 num_val_samples = len(train_data) // k  num_epochs = 500  all_mae_histories = []   for i in range(k):     print('processing fold #', i)     val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]      val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]      partial_train_data = np.concatenate(         [train_data[:i * num_val_samples],          train_data[(i + 1) * num_val_samples:]],          axis=0     )     partial_train_targets = np.concatenate(         [train_targets[:i * num_val_samples],          train_targets[(i + 1) * num_val_samples:]],          axis=0     )     model = build_model()     history = model.fit(         partial_train_data,          partial_train_targets,         validation_data=(val_data, val_targets),         epochs=num_epochs,          batch_size=1,          verbose=0     )     # mae_history = history.history['val_mean_absolute_error']      mae_history = history.history['val_mae']     all_mae_histories.append(mae_history)     print('Training sucessfully in #', i) In\u00a0[\u00a0]: Copied! <pre>average_mae_history = [\n    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)\n]\n</pre> average_mae_history = [     np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs) ] In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(range(1, len(average_mae_history) + 1), average_mae_history) \nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\n\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)  plt.xlabel('Epochs') plt.ylabel('Validation MAE')  plt.show() In\u00a0[\u00a0]: Copied! <pre>def smooth_curve(points, factor=0.9):\n    \n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n\n    return smoothed_points\n\nsmooth_mae_history = smooth_curve(average_mae_history[10:]) \nplt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history) \nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\n\nplt.show()\n</pre> def smooth_curve(points, factor=0.9):          smoothed_points = []     for point in points:         if smoothed_points:             previous = smoothed_points[-1]             smoothed_points.append(previous * factor + point * (1 - factor))         else:             smoothed_points.append(point)      return smoothed_points  smooth_mae_history = smooth_curve(average_mae_history[10:])  plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)  plt.xlabel('Epochs') plt.ylabel('Validation MAE')  plt.show() In\u00a0[\u00a0]: Copied! <pre>model = build_model()\nmodel.fit(\n    train_data, \n    train_targets,\n    epochs=80, \n    batch_size=16, \n    verbose=0\n)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n</pre> model = build_model() model.fit(     train_data,      train_targets,     epochs=80,      batch_size=16,      verbose=0 ) test_mse_score, test_mae_score = model.evaluate(test_data, test_targets) In\u00a0[\u00a0]: Copied! <pre>print(test_mse_score)\nprint(test_mae_score)\n</pre> print(test_mse_score) print(test_mae_score) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"deep_learning/ch03/03_regression/#the-boston-housing-price-dataset","title":"The Boston Housing Price dataset\u00b6","text":"<p>It has relatively few data points: only 506, split between 404 training samples and 102 test samples.</p>"},{"location":"deep_learning/ch03/03_regression/#k-fold","title":"K-fold\u00b6","text":"<p>We have so few data points, then we pick up the K-fold.</p>"},{"location":"deep_learning/ch03/03_regression/#change-the-epochs-into-500","title":"Change the epochs into 500.\u00b6","text":""},{"location":"deep_learning/ch04/01_binary_classification_overfitting/","title":"Two-class Classification","text":"In\u00a0[\u00a0]: Copied! <pre>from keras.datasets import imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n</pre> from keras.datasets import imdb  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) In\u00a0[\u00a0]: Copied! <pre>print(train_data[0])\nprint(train_labels[0])\n</pre> print(train_data[0]) print(train_labels[0]) In\u00a0[\u00a0]: Copied! <pre># max([max(sequence) for sequence in train_data])\n</pre> # max([max(sequence) for sequence in train_data]) In\u00a0[\u00a0]: Copied! <pre>word_index = imdb.get_word_index()\nreverse_word_index = dict(\n    [(value, key) for (key, value) in word_index.items()]\n)\ndecoded_review = ' '.join(\n    [reverse_word_index.get(i - 3, '?') for i in train_data[0]]\n)\n</pre> word_index = imdb.get_word_index() reverse_word_index = dict(     [(value, key) for (key, value) in word_index.items()] ) decoded_review = ' '.join(     [reverse_word_index.get(i - 3, '?') for i in train_data[0]] ) In\u00a0[\u00a0]: Copied! <pre># word_index.items()\n</pre> # word_index.items() In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension)) \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1. \n    return results\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n</pre> import numpy as np  def vectorize_sequences(sequences, dimension=10000):     results = np.zeros((len(sequences), dimension))      for i, sequence in enumerate(sequences):         results[i, sequence] = 1.      return results  x_train = vectorize_sequences(train_data) x_test = vectorize_sequences(test_data) In\u00a0[\u00a0]: Copied! <pre>y_train = np.asarray(train_labels).astype('float32') \ny_test = np.asarray(test_labels).astype('float32')\n</pre> y_train = np.asarray(train_labels).astype('float32')  y_test = np.asarray(test_labels).astype('float32') In\u00a0[\u00a0]: Copied! <pre>from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \nmodel.add(layers.Dense(16, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid'))\n</pre> from keras import models from keras import layers  model = models.Sequential() model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))  model.add(layers.Dense(16, activation='relu'))  model.add(layers.Dense(1, activation='sigmoid')) In\u00a0[\u00a0]: Copied! <pre>model.add(layers.Dense(4, activation='relu', input_shape=(10000,))) \nmodel.add(layers.Dense(4, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid'))\n</pre> model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))  model.add(layers.Dense(4, activation='relu'))  model.add(layers.Dense(1, activation='sigmoid')) In\u00a0[\u00a0]: Copied! <pre>model.compile(\n    optimizer='rmsprop', \n    loss='binary_crossentropy', \n    metrics=['accuracy'],\n)\n</pre> model.compile(     optimizer='rmsprop',      loss='binary_crossentropy',      metrics=['accuracy'], ) In\u00a0[\u00a0]: Copied! <pre>from keras import optimizers \n\nmodel.compile(\n    optimizer=optimizers.RMSprop(lr=0.001), \n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n</pre> from keras import optimizers   model.compile(     optimizer=optimizers.RMSprop(lr=0.001),      loss='binary_crossentropy',     metrics=['accuracy'] ) In\u00a0[\u00a0]: Copied! <pre>from keras import losses\nfrom keras import metrics \n\nmodel.compile(\n    optimizer=optimizers.RMSprop(lr=0.001), \n    loss=losses.binary_crossentropy, \n    metrics=[metrics.binary_accuracy]\n)\n</pre> from keras import losses from keras import metrics   model.compile(     optimizer=optimizers.RMSprop(lr=0.001),      loss=losses.binary_crossentropy,      metrics=[metrics.binary_accuracy] ) In\u00a0[\u00a0]: Copied! <pre>x_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n</pre> x_val = x_train[:10000] partial_x_train = x_train[10000:] y_val = y_train[:10000] partial_y_train = y_train[10000:] In\u00a0[\u00a0]: Copied! <pre>model.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['acc']\n) \nhistory = model.fit(\n    partial_x_train, \n    partial_y_train,\n    epochs=20,\n    batch_size=512, \n    validation_data=(x_val, y_val)\n)\n</pre> model.compile(     optimizer='rmsprop',     loss='binary_crossentropy',     metrics=['acc'] )  history = model.fit(     partial_x_train,      partial_y_train,     epochs=20,     batch_size=512,      validation_data=(x_val, y_val) ) In\u00a0[\u00a0]: Copied! <pre>history_dict = history.history\nhistory_dict.keys()\n</pre> history_dict = history.history history_dict.keys() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Train History\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epoch\")\n\nplt.legend([\"loss\", \"val_loss\"], loc = \"upper left\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"])  plt.title(\"Train History\") plt.ylabel(\"loss\") plt.xlabel(\"Epoch\")  plt.legend([\"loss\", \"val_loss\"], loc = \"upper left\") plt.show() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss') \nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.xlabel('Epochs') \nplt.ylabel('Loss') \nplt.legend() \n\nplt.show()\n</pre> import matplotlib.pyplot as plt  history_dict = history.history loss_values = history_dict['loss'] val_loss_values = history_dict['val_loss']  epochs = range(1, len(loss_values) + 1) plt.plot(epochs, loss_values, 'bo', label='Training loss')  plt.plot(epochs, val_loss_values, 'b', label='Validation loss')  plt.title('Training and validation loss') plt.xlabel('Epochs')  plt.ylabel('Loss')  plt.legend()   plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.clf()\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc'] \nplt.plot(epochs, acc_values, 'bo', label='Training acc') \nplt.plot(epochs, val_acc_values, 'b', label='Validation acc') \nplt.title('Training and validation accuracy') \nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n</pre> plt.clf() acc_values = history_dict['acc'] val_acc_values = history_dict['val_acc']  plt.plot(epochs, acc_values, 'bo', label='Training acc')  plt.plot(epochs, val_acc_values, 'b', label='Validation acc')  plt.title('Training and validation accuracy')  plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \nmodel.add(layers.Dense(16, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid')) \nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\nmodel.fit(\n    x_train, \n    y_train, \n    epochs=4, \n    batch_size=512\n)\n</pre> model = models.Sequential() model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))  model.add(layers.Dense(16, activation='relu'))  model.add(layers.Dense(1, activation='sigmoid'))  model.compile(     optimizer='rmsprop',     loss='binary_crossentropy',     metrics=['accuracy'] ) model.fit(     x_train,      y_train,      epochs=4,      batch_size=512 ) In\u00a0[\u00a0]: Copied! <pre>results = model.evaluate(x_test, y_test)\nprint(results)\n</pre> results = model.evaluate(x_test, y_test) print(results) In\u00a0[\u00a0]: Copied! <pre>model.predict(x_test)\n</pre> model.predict(x_test)"},{"location":"deep_learning/ch04/01_binary_classification_overfitting/#two-class-classification","title":"Two-class Classification\u00b6","text":"<p>Deal with overfitting</p>"},{"location":"deep_learning/ch04/01_binary_classification_overfitting/#import-the-imdb-dataset","title":"Import the IMDB dataset\u00b6","text":""},{"location":"deep_learning/ch04/01_binary_classification_overfitting/#preparing-the-data","title":"Preparing the data.\u00b6","text":""},{"location":"deep_learning/ch04/01_binary_classification_overfitting/#building-the-network","title":"Building the Network\u00b6","text":"<p>relu: meant to zero out negative values</p> <p>sigmoid: \u201csquashes\u201d arbitrary values into the [0, 1] interval</p>"},{"location":"deep_learning/ch04/01_binary_classification_overfitting/#reduce-to-the-smaller-size","title":"Reduce to the smaller size.\u00b6","text":""},{"location":"deep_learning/ch04/01_binary_classification_overfitting/#three-methods-to-compile-the-model","title":"Three methods to compile the model.\u00b6","text":""},{"location":"deep_learning/ch04/01_binary_classification_overfitting/#visualize-the-training-progress","title":"Visualize the training progress.\u00b6","text":""},{"location":"deep_learning/ch04/01_binary_classification_overfitting/#retrain-a-model","title":"Retrain a model\u00b6","text":""},{"location":"deep_learning/ch04/01_binary_classification_overfitting/#further-experiments","title":"Further experiments\u00b6","text":"<p>The following experiments will help convince you that the architecture choices you\u2019ve made are all fairly reasonable, although there\u2019s still room for improvement:</p> <ul> <li>You used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.</li> <li>Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.</li> <li>Try using the mse loss function instead of binary_crossentropy.</li> <li>Try using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu.</li> </ul>"},{"location":"deep_learning/ch04/02_evaluate/","title":"SIMPLE HOLD-OUT VALIDATION","text":"<pre><code>num_validation_samples = 10000 \nnp.random.shuffle(data)\nvalidation_data = data[:num_validation_samples] \ndata = data[num_validation_samples:] \ntraining_data = data[:]\nmodel = get_model()\nmodel.train(training_data)\nvalidation_score = model.evaluate(validation_data) # At this point you can tune your model,\n# retrain it, evaluate it, tune it again...\nmodel = get_model() \nmodel.train(np.concatenate([training_data, validation_data]))\ntest_score = model.evaluate(test_data)\n</code></pre>"},{"location":"deep_learning/ch04/02_evaluate/#k-fold-validation","title":"K-Fold Validation","text":"<pre><code>k=4\nnum_validation_samples = len(data) // k \nnp.random.shuffle(data) \nvalidation_scores = []\n\nfor fold in range(k):\n    validation_data = data[num_validation_samples * fold: num_validation_samples * (fold + 1)]\n    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]\n    model = get_model()\n    model.train(training_data)\n    validation_score = model.evaluate(validation_data) \n    validation_scores.append(validation_score)\n    validation_score = np.average(validation_scores) \n    model = get_model()\n    model.train(data)\n    test_score = model.evaluate(test_data)\n</code></pre>"},{"location":"deep_learning/ch04/02_evaluate/#iterated-k-fold-validation-with-shuffling","title":"ITERATED K-FOLD VALIDATION WITH SHUFFLING","text":""},{"location":"deep_learning/ch05/01_mnist/","title":"mnist with CNN","text":"<p>Import the package</p> In\u00a0[\u00a0]: Copied! <pre>from keras.datasets import mnist\nfrom keras.utils import to_categorical\nfrom keras import layers \nfrom keras import models\n</pre> from keras.datasets import mnist from keras.utils import to_categorical from keras import layers  from keras import models <p>Load <code>mnist</code> dataset</p> In\u00a0[\u00a0]: Copied! <pre>(train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n\ntrain_images = train_images.reshape((60000, 28, 28, 1))\ntrain_images = train_images.astype('float32') / 255\ntest_images = test_images.reshape((10000, 28, 28, 1))\ntest_images = test_images.astype('float32') / 255 \ntrain_labels = to_categorical(train_labels) \ntest_labels = to_categorical(test_labels) \n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\n\nmodel.add(layers.Flatten()) \nmodel.add(layers.Dense(64, activation='relu')) \nmodel.add(layers.Dense(10, activation='softmax'))\nmodel.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.fit(train_images, train_labels, epochs=5, batch_size=64)\n</pre> (train_images, train_labels), (test_images, test_labels) = mnist.load_data()   train_images = train_images.reshape((60000, 28, 28, 1)) train_images = train_images.astype('float32') / 255 test_images = test_images.reshape((10000, 28, 28, 1)) test_images = test_images.astype('float32') / 255  train_labels = to_categorical(train_labels)  test_labels = to_categorical(test_labels)   model = models.Sequential() model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))  model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation='relu'))  model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation='relu'))  model.add(layers.Flatten())  model.add(layers.Dense(64, activation='relu'))  model.add(layers.Dense(10, activation='softmax')) model.compile(     optimizer='rmsprop',     loss='categorical_crossentropy',     metrics=['accuracy'] ) model.fit(train_images, train_labels, epochs=5, batch_size=64)"},{"location":"deep_learning/ch05/01_mnist/#mnist-with-cnn","title":"<code>mnist</code> with CNN\u00b6","text":""},{"location":"machine_learning/ch01/01_evaluation_index/","title":"\u4e00\u3001\u6a5f\u5668\u5b78\u7fd2\u8a55\u4f30\u6307\u6a19\u9078\u5b9a","text":"In\u00a0[\u00a0]: Copied! <pre>from sklearn import metrics, datasets\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\nimport numpy as np\n</pre> from sklearn import metrics, datasets from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  import matplotlib.pyplot as plt from matplotlib.pylab import rcParams import numpy as np <p>\u6211\u5011\u96a8\u6a5f\u751f\u6210(X, y)\u8cc7\u6599\uff0c\u7136\u5f8c\u4f7f\u7528\u7dda\u6027\u56de\u6b78\u6a21\u578b\u505a\u9810\u6e2c\uff0c\u518d\u4f7f\u7528 MAE, MSE, R-square \u8a55\u4f30</p> In\u00a0[\u00a0]: Copied! <pre>X, y = datasets.make_regression(n_features=1, random_state=42, noise=100) # \u751f\u6210\u8cc7\u6599\nmodel = LinearRegression() # \u5efa\u7acb\u56de\u6b78\u6a21\u578b\nmodel.fit(X, y) # \u5c07\u8cc7\u6599\u653e\u9032\u6a21\u578b\u8a13\u7df4\nprediction = model.predict(X) # \u9032\u884c\u9810\u6e2c\nmae = metrics.mean_absolute_error(prediction, y) # \u4f7f\u7528 MAE \u8a55\u4f30\nmse = metrics.mean_squared_error(prediction, y) # \u4f7f\u7528 MSE \u8a55\u4f30\nr2 = metrics.r2_score(prediction, y) # \u4f7f\u7528 r-square \u8a55\u4f30\nprint(\"MAE: \", mae)\nprint(\"MSE: \", mse)\nprint(\"R-square: \", r2)\n</pre> X, y = datasets.make_regression(n_features=1, random_state=42, noise=100) # \u751f\u6210\u8cc7\u6599 model = LinearRegression() # \u5efa\u7acb\u56de\u6b78\u6a21\u578b model.fit(X, y) # \u5c07\u8cc7\u6599\u653e\u9032\u6a21\u578b\u8a13\u7df4 prediction = model.predict(X) # \u9032\u884c\u9810\u6e2c mae = metrics.mean_absolute_error(prediction, y) # \u4f7f\u7528 MAE \u8a55\u4f30 mse = metrics.mean_squared_error(prediction, y) # \u4f7f\u7528 MSE \u8a55\u4f30 r2 = metrics.r2_score(prediction, y) # \u4f7f\u7528 r-square \u8a55\u4f30 print(\"MAE: \", mae) print(\"MSE: \", mse) print(\"R-square: \", r2) In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X,y)\nplt.show()\n</pre> plt.scatter(X,y) plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.scatter(X, prediction)\nplt.show()\n</pre> plt.scatter(X, prediction) plt.show() In\u00a0[\u00a0]: Copied! <pre>cancer = datasets.load_breast_cancer() # \u6211\u5011\u4f7f\u7528 sklearn \u5167\u542b\u7684\u4e73\u764c\u8cc7\u6599\u96c6\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=50, random_state=0)\n</pre> cancer = datasets.load_breast_cancer() # \u6211\u5011\u4f7f\u7528 sklearn \u5167\u542b\u7684\u4e73\u764c\u8cc7\u6599\u96c6 X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=50, random_state=0) In\u00a0[\u00a0]: Copied! <pre>print(y_test) # \u6e2c\u8a66\u96c6\u4e2d\u7684 label\n</pre> print(y_test) # \u6e2c\u8a66\u96c6\u4e2d\u7684 label In\u00a0[\u00a0]: Copied! <pre>print(X_train)\n</pre> print(X_train) In\u00a0[\u00a0]: Copied! <pre>y_pred = np.random.random((50,)) # \u6211\u5011\u5148\u96a8\u6a5f\u751f\u6210 50 \u7b46\u9810\u6e2c\u503c\uff0c\u7bc4\u570d\u90fd\u5728 0~1 \u4e4b\u9593\uff0c\u4ee3\u8868\u6a5f\u7387\u503c\n</pre> y_pred = np.random.random((50,)) # \u6211\u5011\u5148\u96a8\u6a5f\u751f\u6210 50 \u7b46\u9810\u6e2c\u503c\uff0c\u7bc4\u570d\u90fd\u5728 0~1 \u4e4b\u9593\uff0c\u4ee3\u8868\u6a5f\u7387\u503c In\u00a0[\u00a0]: Copied! <pre>print(y_pred)\n</pre> print(y_pred) In\u00a0[\u00a0]: Copied! <pre>auc = metrics.roc_auc_score(y_test, y_pred) # \u4f7f\u7528 roc_auc_score \u4f86\u8a55\u4f30\u3002 **\u9019\u908a\u7279\u5225\u6ce8\u610f y_pred \u5fc5\u9808\u8981\u653e\u6a5f\u7387\u503c\u9032\u53bb!**\nprint(\"AUC: \", auc) # \u5f97\u5230\u7d50\u679c\u7d04 0.5\uff0c\u8207\u4e82\u731c\u7684\u7d50\u679c\u76f8\u8fd1\uff0c\u56e0\u70ba\u6211\u5011\u7684\u9810\u6e2c\u503c\u662f\u7528\u96a8\u6a5f\u751f\u6210\u7684\n</pre> auc = metrics.roc_auc_score(y_test, y_pred) # \u4f7f\u7528 roc_auc_score \u4f86\u8a55\u4f30\u3002 **\u9019\u908a\u7279\u5225\u6ce8\u610f y_pred \u5fc5\u9808\u8981\u653e\u6a5f\u7387\u503c\u9032\u53bb!** print(\"AUC: \", auc) # \u5f97\u5230\u7d50\u679c\u7d04 0.5\uff0c\u8207\u4e82\u731c\u7684\u7d50\u679c\u76f8\u8fd1\uff0c\u56e0\u70ba\u6211\u5011\u7684\u9810\u6e2c\u503c\u662f\u7528\u96a8\u6a5f\u751f\u6210\u7684 In\u00a0[\u00a0]: Copied! <pre>threshold = 0.5 \ny_pred_binarized = np.where(y_pred&gt;threshold, 1, 0) # \u4f7f\u7528 np.where \u51fd\u6578, \u5c07 y_pred &gt; 0.5 \u7684\u503c\u8b8a\u70ba 1\uff0c\u5c0f\u65bc 0.5 \u7684\u70ba 0\nf1 = metrics.f1_score(y_test, y_pred_binarized) # \u4f7f\u7528 F1-Score \u8a55\u4f30\nprecision = metrics.precision_score(y_test, y_pred_binarized) # \u4f7f\u7528 Precision \u8a55\u4f30\nrecall  = metrics.recall_score(y_test, y_pred_binarized) # \u4f7f\u7528 recall \u8a55\u4f30\nprint(\"F1-Score: \", f1) \nprint(\"Precision: \", precision)\nprint(\"Recall: \", recall)\n</pre> threshold = 0.5  y_pred_binarized = np.where(y_pred&gt;threshold, 1, 0) # \u4f7f\u7528 np.where \u51fd\u6578, \u5c07 y_pred &gt; 0.5 \u7684\u503c\u8b8a\u70ba 1\uff0c\u5c0f\u65bc 0.5 \u7684\u70ba 0 f1 = metrics.f1_score(y_test, y_pred_binarized) # \u4f7f\u7528 F1-Score \u8a55\u4f30 precision = metrics.precision_score(y_test, y_pred_binarized) # \u4f7f\u7528 Precision \u8a55\u4f30 recall  = metrics.recall_score(y_test, y_pred_binarized) # \u4f7f\u7528 recall \u8a55\u4f30 print(\"F1-Score: \", f1)  print(\"Precision: \", precision) print(\"Recall: \", recall)"},{"location":"machine_learning/ch01/01_evaluation_index/","title":"\u4e00\u3001\u6a5f\u5668\u5b78\u7fd2\u8a55\u4f30\u6307\u6a19\u9078\u5b9a\u00b6","text":""},{"location":"machine_learning/ch01/01_evaluation_index/","title":"[\u6559\u5b78\u76ee\u6a19]\u00b6","text":"<p>\u5b78\u7fd2 sklearn \u4e2d\uff0c\u5404\u7a2e\u8a55\u4f30\u6307\u6a19\u7684\u4f7f\u7528\u8207\u610f\u7fa9</p>"},{"location":"machine_learning/ch01/01_evaluation_index/","title":"[\u7bc4\u4f8b\u91cd\u9ede]\u00b6","text":"<p>\u6ce8\u610f\u89c0\u5bdf\u5404\u6307\u6a19\u7684\u6578\u503c\u7bc4\u570d\uff0c\u4ee5\u53ca\u8f38\u5165\u51fd\u6578\u4e2d\u7684\u8cc7\u6599\u683c\u5f0f</p>"},{"location":"machine_learning/ch01/01_evaluation_index/#import","title":"import \u9700\u8981\u7684\u5957\u4ef6\u00b6","text":""},{"location":"machine_learning/ch01/01_evaluation_index/","title":"\u56de\u6b78\u554f\u984c\u00b6","text":"<p>\u5e38\u898b\u7684\u8a55\u4f30\u6307\u6a19\u6709</p> <ul> <li>MAE</li> <li>MSE</li> <li>R-square</li> </ul>"},{"location":"machine_learning/ch01/01_evaluation_index/","title":"\u5206\u985e\u554f\u984c\u00b6","text":"<p>\u5e38\u898b\u7684\u8a55\u4f30\u6307\u6a19\u6709</p> <ul> <li>AUC</li> <li>F1-Score (Precision, Recall)</li> </ul>"},{"location":"machine_learning/ch01/01_evaluation_index/#auc","title":"AUC\u00b6","text":""},{"location":"machine_learning/ch01/01_evaluation_index/#f1-score","title":"F1-Score\u00b6","text":""},{"location":"machine_learning/ch01/01_evaluation_index/","title":"[\u672c\u7bc0\u91cd\u9ede]\u00b6","text":"<p>\u4e86\u89e3 F1-score \u7684\u516c\u5f0f\u610f\u7fa9\uff0c\u4e26\u8a66\u8457\u7406\u89e3\u7a0b\u5f0f\u78bc</p>"},{"location":"machine_learning/ch01/01_evaluation_index/","title":"\u7df4\u7fd2\u00b6","text":"<p>\u8acb\u53c3\u8003 F1-score \u7684\u516c\u5f0f\u8207\u539f\u59cb\u78bc\uff0c\u8a66\u8457\u5beb\u51fa F2-Score \u7684\u8a08\u7b97\u51fd\u6578</p>"},{"location":"machine_learning/ch01/02_regression_model/","title":"\u4e8c\u3001Regression \u6a21\u578b","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn import datasets, linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score, accuracy_score In\u00a0[\u00a0]: Copied! <pre># \u8b80\u53d6\u7cd6\u5c3f\u75c5\u8cc7\u6599\u96c6\ndiabetes = datasets.load_diabetes()\n\n# \u70ba\u65b9\u4fbf\u8996\u89ba\u5316\uff0c\u6211\u5011\u53ea\u4f7f\u7528\u8cc7\u6599\u96c6\u4e2d\u7684 1 \u500b feature (column)\nX = diabetes.data[:, np.newaxis, 2]\nprint(\"Data shape: \", X.shape) # \u53ef\u4ee5\u770b\u898b\u6709 442 \u7b46\u8cc7\u6599\u8207\u6211\u5011\u53d6\u51fa\u7684\u5176\u4e2d\u4e00\u500b feature\n\n# \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\nx_train, x_test, y_train, y_test = train_test_split(X, diabetes.target, test_size=0.1, random_state=4)\n\n# \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b\nregr = linear_model.LinearRegression()\n\n# \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4\nregr.fit(x_train, y_train)\n\n# \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c\ny_pred = regr.predict(x_test)\n</pre> # \u8b80\u53d6\u7cd6\u5c3f\u75c5\u8cc7\u6599\u96c6 diabetes = datasets.load_diabetes()  # \u70ba\u65b9\u4fbf\u8996\u89ba\u5316\uff0c\u6211\u5011\u53ea\u4f7f\u7528\u8cc7\u6599\u96c6\u4e2d\u7684 1 \u500b feature (column) X = diabetes.data[:, np.newaxis, 2] print(\"Data shape: \", X.shape) # \u53ef\u4ee5\u770b\u898b\u6709 442 \u7b46\u8cc7\u6599\u8207\u6211\u5011\u53d6\u51fa\u7684\u5176\u4e2d\u4e00\u500b feature  # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6 x_train, x_test, y_train, y_test = train_test_split(X, diabetes.target, test_size=0.1, random_state=4)  # \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b regr = linear_model.LinearRegression()  # \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4 regr.fit(x_train, y_train)  # \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c y_pred = regr.predict(x_test) In\u00a0[\u00a0]: Copied! <pre>diabetes.data.shape\n</pre> diabetes.data.shape In\u00a0[\u00a0]: Copied! <pre># \u53ef\u4ee5\u770b\u56de\u6b78\u6a21\u578b\u7684\u53c3\u6578\u503c\nprint('Coefficients: ', regr.coef_)\n\n# \u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u5dee\u8ddd\uff0c\u4f7f\u7528 MSE\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, y_pred))\n</pre> # \u53ef\u4ee5\u770b\u56de\u6b78\u6a21\u578b\u7684\u53c3\u6578\u503c print('Coefficients: ', regr.coef_)  # \u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u5dee\u8ddd\uff0c\u4f7f\u7528 MSE print(\"Mean squared error: %.2f\"       % mean_squared_error(y_test, y_pred)) In\u00a0[\u00a0]: Copied! <pre># \u756b\u51fa\u56de\u6b78\u6a21\u578b\u8207\u5be6\u969b\u8cc7\u6599\u7684\u5206\u4f48\nplt.scatter(x_test, y_test,  color='black')\nplt.plot(x_test, y_pred, color='blue', linewidth=3)\n\nplt.show()\n</pre> # \u756b\u51fa\u56de\u6b78\u6a21\u578b\u8207\u5be6\u969b\u8cc7\u6599\u7684\u5206\u4f48 plt.scatter(x_test, y_test,  color='black') plt.plot(x_test, y_pred, color='blue', linewidth=3)  plt.show() In\u00a0[\u00a0]: Copied! <pre># \u8b80\u53d6\u9cf6\u5c3e\u82b1\u8cc7\u6599\u96c6\niris = datasets.load_iris()\n\n# \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\nx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, random_state=4)\n\n# \u5efa\u7acb\u6a21\u578b\nlogreg = linear_model.LogisticRegression()\n\n# \u8a13\u7df4\u6a21\u578b\nlogreg.fit(x_train, y_train)\n\n# \u9810\u6e2c\u6e2c\u8a66\u96c6\ny_pred = logreg.predict(x_test)\n</pre> # \u8b80\u53d6\u9cf6\u5c3e\u82b1\u8cc7\u6599\u96c6 iris = datasets.load_iris()  # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6 x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, random_state=4)  # \u5efa\u7acb\u6a21\u578b logreg = linear_model.LogisticRegression()  # \u8a13\u7df4\u6a21\u578b logreg.fit(x_train, y_train)  # \u9810\u6e2c\u6e2c\u8a66\u96c6 y_pred = logreg.predict(x_test) In\u00a0[\u00a0]: Copied! <pre>acc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", acc)\n</pre> acc = accuracy_score(y_test, y_pred) print(\"Accuracy: \", acc) In\u00a0[\u00a0]: Copied! <pre>wine = datasets.load_wine()\nboston = datasets.load_boston()\nbreast_cancer = datasets.load_breast_cancer()\n</pre> wine = datasets.load_wine() boston = datasets.load_boston() breast_cancer = datasets.load_breast_cancer()"},{"location":"machine_learning/ch01/02_regression_model/#regression","title":"\u4e8c\u3001Regression \u6a21\u578b\u00b6","text":""},{"location":"machine_learning/ch01/02_regression_model/","title":"[\u6559\u5b78\u91cd\u9ede]\u00b6","text":"<p>\u5b78\u7fd2\u4f7f\u7528 sklearn \u4e2d\u7684 linear regression \u6a21\u578b\uff0c\u4e26\u7406\u89e3\u5404\u9805\u53c3\u6578\u7684\u610f\u7fa9</p>"},{"location":"machine_learning/ch01/02_regression_model/","title":"[\u7bc4\u4f8b\u91cd\u9ede]\u00b6","text":"<p>\u89c0\u5bdf\u4e1f\u9032\u6a21\u578b\u8a13\u7df4\u7684\u8cc7\u6599\u683c\u5f0f\uff0c\u8f38\u5165 linear regression \u8207 Logistic regression \u7684\u8cc7\u6599\u6709\u751a\u9ebc\u4e0d\u540c?</p>"},{"location":"machine_learning/ch01/02_regression_model/#import","title":"import \u9700\u8981\u7684\u5957\u4ef6\u00b6","text":""},{"location":"machine_learning/ch01/02_regression_model/#linear-regression","title":"Linear regression\u00b6","text":""},{"location":"machine_learning/ch01/02_regression_model/#logistics-regression","title":"Logistics regression\u00b6","text":""},{"location":"machine_learning/ch01/02_regression_model/","title":"[\u7df4\u7fd2\u91cd\u9ede]\u00b6","text":"<p>\u4e86\u89e3\u5176\u4ed6\u8cc7\u6599\u96c6\u7684\u4f7f\u7528\u65b9\u6cd5\uff0c\u5982\u4f55\u5c07\u8cc7\u6599\u6b63\u78ba\u5730\u9001\u9032\u6a21\u578b\u8a13\u7df4</p>"},{"location":"machine_learning/ch01/02_regression_model/","title":"\u7df4\u7fd2\u6642\u9593\u00b6","text":"<p>\u8a66\u8457\u4f7f\u7528 sklearn datasets \u7684\u5176\u4ed6\u8cc7\u6599\u96c6 (wine, boston, ...)\uff0c\u4f86\u8a13\u7df4\u81ea\u5df1\u7684\u7dda\u6027\u8ff4\u6b78\u6a21\u578b\u3002</p>"},{"location":"machine_learning/ch01/02_regression_model/#hint-label","title":"HINT: \u6ce8\u610f label \u7684\u578b\u614b\uff0c\u78ba\u5b9a\u8cc7\u6599\u96c6\u7684\u76ee\u6a19\u662f\u5206\u985e\u9084\u662f\u56de\u6b78\uff0c\u518d\u4f7f\u7528\u6b63\u78ba\u7684\u6a21\u578b\u8a13\u7df4\uff01\u00b6","text":""},{"location":"machine_learning/ch01/03_lasso_ridge/","title":"\u4e09\u3001Lasso\u3001Ridge Regression \u6a21\u578b","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn import datasets, linear_model from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score In\u00a0[\u00a0]: Copied! <pre># \u8b80\u53d6\u7cd6\u5c3f\u75c5\u8cc7\u6599\u96c6\ndiabetes = datasets.load_diabetes()\n\n# \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\nx_train, x_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=4)\n\n# \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b\nregr = linear_model.LinearRegression()\n\n# \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4\nregr.fit(x_train, y_train)\n\n# \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c\ny_pred = regr.predict(x_test)\n</pre> # \u8b80\u53d6\u7cd6\u5c3f\u75c5\u8cc7\u6599\u96c6 diabetes = datasets.load_diabetes()  # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6 x_train, x_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=4)  # \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b regr = linear_model.LinearRegression()  # \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4 regr.fit(x_train, y_train)  # \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c y_pred = regr.predict(x_test) In\u00a0[\u00a0]: Copied! <pre>x_train.shape\n</pre> x_train.shape In\u00a0[\u00a0]: Copied! <pre>x_train[1]\n</pre> x_train[1] <p>y = x1w1 + x2w2 + .... + x10*w10 + b</p> In\u00a0[\u00a0]: Copied! <pre>print(regr.coef_)\n</pre> print(regr.coef_) In\u00a0[\u00a0]: Copied! <pre># \u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u5dee\u8ddd\uff0c\u4f7f\u7528 MSE\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, y_pred))\n</pre> # \u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u5dee\u8ddd\uff0c\u4f7f\u7528 MSE print(\"Mean squared error: %.2f\"       % mean_squared_error(y_test, y_pred)) In\u00a0[\u00a0]: Copied! <pre># \u8b80\u53d6\u7cd6\u5c3f\u75c5\u8cc7\u6599\u96c6\ndiabetes = datasets.load_diabetes()\n\n# \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\nx_train, x_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=4)\n\n# \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b\nlasso = linear_model.Lasso(alpha=1.0)\n\n# \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4\nlasso.fit(x_train, y_train)\n\n# \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c\ny_pred = lasso.predict(x_test)\n</pre> # \u8b80\u53d6\u7cd6\u5c3f\u75c5\u8cc7\u6599\u96c6 diabetes = datasets.load_diabetes()  # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6 x_train, x_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=4)  # \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b lasso = linear_model.Lasso(alpha=1.0)  # \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4 lasso.fit(x_train, y_train)  # \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c y_pred = lasso.predict(x_test) In\u00a0[\u00a0]: Copied! <pre># \u5370\u51fa\u5404\u7279\u5fb5\u5c0d\u61c9\u7684\u4fc2\u6578\uff0c\u53ef\u4ee5\u770b\u5230\u8a31\u591a\u4fc2\u6578\u90fd\u8b8a\u6210 0\uff0cLasso Regression \u7684\u78ba\u53ef\u4ee5\u505a\u7279\u5fb5\u9078\u53d6\nlasso.coef_\n</pre> # \u5370\u51fa\u5404\u7279\u5fb5\u5c0d\u61c9\u7684\u4fc2\u6578\uff0c\u53ef\u4ee5\u770b\u5230\u8a31\u591a\u4fc2\u6578\u90fd\u8b8a\u6210 0\uff0cLasso Regression \u7684\u78ba\u53ef\u4ee5\u505a\u7279\u5fb5\u9078\u53d6 lasso.coef_ In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># \u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u5dee\u8ddd\uff0c\u4f7f\u7528 MSE\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, y_pred))\n</pre> # \u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u5dee\u8ddd\uff0c\u4f7f\u7528 MSE print(\"Mean squared error: %.2f\"       % mean_squared_error(y_test, y_pred)) In\u00a0[\u00a0]: Copied! <pre># \u8b80\u53d6\u7cd6\u5c3f\u75c5\u8cc7\u6599\u96c6\ndiabetes = datasets.load_diabetes()\n\n# \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\nx_train, x_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=4)\n\n# \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b\nridge = linear_model.Ridge(alpha=10)\n\n# \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4\nridge.fit(x_train, y_train)\n\n# \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c\ny_pred = regr.predict(x_test)\n</pre> # \u8b80\u53d6\u7cd6\u5c3f\u75c5\u8cc7\u6599\u96c6 diabetes = datasets.load_diabetes()  # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6 x_train, x_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=4)  # \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b ridge = linear_model.Ridge(alpha=10)  # \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4 ridge.fit(x_train, y_train)  # \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c y_pred = regr.predict(x_test) In\u00a0[\u00a0]: Copied! <pre># \u5370\u51fa Ridge \u7684\u53c3\u6578\uff0c\u53ef\u4ee5\u5f88\u660e\u986f\u770b\u5230\u6bd4\u8d77 Linear Regression\uff0c\u53c3\u6578\u7684\u6578\u503c\u90fd\u660e\u986f\u5c0f\u4e86\u8a31\u591a\nprint(ridge.coef_)\n</pre> # \u5370\u51fa Ridge \u7684\u53c3\u6578\uff0c\u53ef\u4ee5\u5f88\u660e\u986f\u770b\u5230\u6bd4\u8d77 Linear Regression\uff0c\u53c3\u6578\u7684\u6578\u503c\u90fd\u660e\u986f\u5c0f\u4e86\u8a31\u591a print(ridge.coef_) In\u00a0[\u00a0]: Copied! <pre># \u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u5dee\u8ddd\uff0c\u4f7f\u7528 MSE\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, y_pred))\n</pre> # \u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u5dee\u8ddd\uff0c\u4f7f\u7528 MSE print(\"Mean squared error: %.2f\"       % mean_squared_error(y_test, y_pred)) <p>\u53ef\u4ee5\u770b\u898b LASSO \u8207 Ridge \u7684\u7d50\u679c\u4e26\u6c92\u6709\u6bd4\u539f\u672c\u7684\u7dda\u6027\u56de\u6b78\u4f86\u5f97\u597d\uff0c \u9019\u662f\u56e0\u70ba\u76ee\u6a19\u51fd\u6578\u88ab\u52a0\u4e0a\u4e86\u6b63\u898f\u5316\u51fd\u6578\uff0c\u8b93\u6a21\u578b\u4e0d\u80fd\u904e\u65bc\u8907\u96dc\uff0c\u76f8\u7576\u65bc\u9650\u5236\u6a21\u578b\u64ec\u548c\u8cc7\u6599\u7684\u80fd\u529b\u3002\u56e0\u6b64\u82e5\u6c92\u6709\u767c\u73fe Over-fitting \u7684\u60c5\u6cc1\uff0c\u662f\u53ef\u4ee5\u4e0d\u9700\u8981\u4e00\u958b\u59cb\u5c31\u52a0\u4e0a\u592a\u5f37\u7684\u6b63\u898f\u5316\u7684\u3002</p>"},{"location":"machine_learning/ch01/03_lasso_ridge/#lassoridge-regression","title":"\u4e09\u3001Lasso\u3001Ridge Regression \u6a21\u578b\u00b6","text":""},{"location":"machine_learning/ch01/03_lasso_ridge/#import","title":"import \u9700\u8981\u7684\u5957\u4ef6\u00b6","text":""},{"location":"machine_learning/ch01/03_lasso_ridge/#lasso","title":"LASSO\u00b6","text":""},{"location":"machine_learning/ch01/03_lasso_ridge/#ridge","title":"Ridge\u00b6","text":""},{"location":"machine_learning/ch01/03_lasso_ridge/","title":"\u7df4\u7fd2\u6642\u9593\u00b6","text":"<p>\u8acb\u4f7f\u7528\u5176\u4ed6\u8cc7\u6599\u96c6 (boston, wine)\uff0c\u4e26\u8abf\u6574\u4e0d\u540c\u7684 alpha \u4f86\u89c0\u5bdf\u6a21\u578b\u8a13\u7df4\u7684\u60c5\u5f62\u3002</p>"},{"location":"machine_learning/ch01/04_decision_tree/","title":"\u56db\u3001\u6c7a\u7b56\u6a39","text":"In\u00a0[\u00a0]: Copied! <pre>from sklearn import datasets, metrics\n\n# \u5982\u679c\u662f\u5206\u985e\u554f\u984c\uff0c\u8acb\u4f7f\u7528 DecisionTreeClassifier\uff0c\u82e5\u70ba\u56de\u6b78\u554f\u984c\uff0c\u8acb\u4f7f\u7528 DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\n</pre> from sklearn import datasets, metrics  # \u5982\u679c\u662f\u5206\u985e\u554f\u984c\uff0c\u8acb\u4f7f\u7528 DecisionTreeClassifier\uff0c\u82e5\u70ba\u56de\u6b78\u554f\u984c\uff0c\u8acb\u4f7f\u7528 DecisionTreeRegressor from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.model_selection import train_test_split In\u00a0[\u00a0]: Copied! <pre># \u8b80\u53d6\u9cf6\u5c3e\u82b1\u8cc7\u6599\u96c6\niris = datasets.load_iris()\n\n# \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\nx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n\n# \u5efa\u7acb\u6a21\u578b\nclf = DecisionTreeClassifier()\n\n# \u8a13\u7df4\u6a21\u578b\nclf.fit(x_train, y_train)\n\n# \u9810\u6e2c\u6e2c\u8a66\u96c6\ny_pred = clf.predict(x_test)\n</pre> # \u8b80\u53d6\u9cf6\u5c3e\u82b1\u8cc7\u6599\u96c6 iris = datasets.load_iris()  # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6 x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)  # \u5efa\u7acb\u6a21\u578b clf = DecisionTreeClassifier()  # \u8a13\u7df4\u6a21\u578b clf.fit(x_train, y_train)  # \u9810\u6e2c\u6e2c\u8a66\u96c6 y_pred = clf.predict(x_test) In\u00a0[\u00a0]: Copied! <pre>acc = metrics.accuracy_score(y_test, y_pred)\nprint(\"Acuuracy: \", acc)\n</pre> acc = metrics.accuracy_score(y_test, y_pred) print(\"Acuuracy: \", acc) In\u00a0[\u00a0]: Copied! <pre>print(iris.feature_names)\n</pre> print(iris.feature_names) In\u00a0[\u00a0]: Copied! <pre>print(\"Feature importance: \", clf.feature_importances_)\n</pre> print(\"Feature importance: \", clf.feature_importances_)"},{"location":"machine_learning/ch01/04_decision_tree/","title":"\u56db\u3001\u6c7a\u7b56\u6a39\u00b6","text":""},{"location":"machine_learning/ch01/04_decision_tree/","title":"[\u7bc4\u4f8b\u91cd\u9ede]\u00b6","text":"<p>\u4e86\u89e3\u6a5f\u5668\u5b78\u7fd2\u5efa\u6a21\u7684\u6b65\u9a5f\u3001\u8cc7\u6599\u578b\u614b\u4ee5\u53ca\u8a55\u4f30\u7d50\u679c\u7b49\u6d41\u7a0b</p>"},{"location":"machine_learning/ch01/04_decision_tree/#import","title":"import \u9700\u8981\u7684\u5957\u4ef6\u00b6","text":""},{"location":"machine_learning/ch01/04_decision_tree/","title":"\u5efa\u7acb\u6a21\u578b\u56db\u6b65\u9a5f\u00b6","text":"<p>\u5728 Scikit-learn \u4e2d\uff0c\u5efa\u7acb\u4e00\u500b\u6a5f\u5668\u5b78\u7fd2\u7684\u6a21\u578b\u5176\u5be6\u975e\u5e38\u7c21\u55ae\uff0c\u6d41\u7a0b\u5927\u7565\u662f\u4ee5\u4e0b\u56db\u500b\u6b65\u9a5f</p> <ol> <li>\u8b80\u9032\u8cc7\u6599\uff0c\u4e26\u6aa2\u67e5\u8cc7\u6599\u7684 shape (\u6709\u591a\u5c11 samples (rows), \u591a\u5c11 features (columns)\uff0clabel \u7684\u578b\u614b\u662f\u4ec0\u9ebc\uff1f)<ul> <li>\u8b80\u53d6\u8cc7\u6599\u7684\u65b9\u6cd5\uff1a<ul> <li>\u4f7f\u7528 pandas \u8b80\u53d6 .csv \u6a94\uff1apd.read_csv</li> <li>\u4f7f\u7528 numpy \u8b80\u53d6 .txt \u6a94\uff1anp.loadtxt</li> <li>\u4f7f\u7528 Scikit-learn \u5167\u5efa\u7684\u8cc7\u6599\u96c6\uff1asklearn.datasets.load_xxx</li> </ul> </li> <li>\u6aa2\u67e5\u8cc7\u6599\u6578\u91cf\uff1adata.shape (data should be np.array or dataframe)</li> </ul> </li> <li>\u5c07\u8cc7\u6599\u5207\u70ba\u8a13\u7df4 (train) / \u6e2c\u8a66 (test)<ul> <li>train_test_split(data)</li> </ul> </li> <li>\u5efa\u7acb\u6a21\u578b\uff0c\u5c07\u8cc7\u6599 fit \u9032\u6a21\u578b\u958b\u59cb\u8a13\u7df4<ul> <li>clf = DecisionTreeClassifier()</li> <li>clf.fit(x_train, y_train)</li> </ul> </li> <li>\u5c07\u6e2c\u8a66\u8cc7\u6599 (features) \u653e\u9032\u8a13\u7df4\u597d\u7684\u6a21\u578b\u4e2d\uff0c\u5f97\u5230 prediction\uff0c\u8207\u6e2c\u8a66\u8cc7\u6599\u7684 label (y_test) \u505a\u8a55\u4f30<ul> <li>clf.predict(x_test)</li> <li>accuracy_score(y_test, y_pred)</li> <li>f1_score(y_test, y_pred)</li> </ul> </li> </ol>"},{"location":"machine_learning/ch01/04_decision_tree/","title":"\u7df4\u7fd2\u00b6","text":"<ol> <li>\u8a66\u8457\u8abf\u6574 DecisionTreeClassifier(...) \u4e2d\u7684\u53c3\u6578\uff0c\u4e26\u89c0\u5bdf\u662f\u5426\u6703\u6539\u8b8a\u7d50\u679c\uff1f</li> <li>\u6539\u7528\u5176\u4ed6\u8cc7\u6599\u96c6 (boston, wine)\uff0c\u4e26\u8207\u56de\u6b78\u6a21\u578b\u7684\u7d50\u679c\u9032\u884c\u6bd4\u8f03</li> </ol>"},{"location":"machine_learning/ch01/05_random_forest/","title":"\u4e94\u3001\u96a8\u6a5f\u68ee\u6797","text":"In\u00a0[\u00a0]: Copied! <pre>from sklearn import datasets, metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n</pre> from sklearn import datasets, metrics from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split In\u00a0[\u00a0]: Copied! <pre># \u8b80\u53d6\u9cf6\u5c3e\u82b1\u8cc7\u6599\u96c6\niris = datasets.load_iris()\n\n# \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\nx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n\n# \u5efa\u7acb\u6a21\u578b (\u4f7f\u7528 20 \u9846\u6a39\uff0c\u6bcf\u68f5\u6a39\u7684\u6700\u5927\u6df1\u5ea6\u70ba 4)\nclf = RandomForestClassifier(n_estimators=10000, max_depth=10)\n\n# \u8a13\u7df4\u6a21\u578b\nclf.fit(x_train, y_train)\n\n# \u9810\u6e2c\u6e2c\u8a66\u96c6\ny_pred = clf.predict(x_test)\n</pre> # \u8b80\u53d6\u9cf6\u5c3e\u82b1\u8cc7\u6599\u96c6 iris = datasets.load_iris()  # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6 x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)  # \u5efa\u7acb\u6a21\u578b (\u4f7f\u7528 20 \u9846\u6a39\uff0c\u6bcf\u68f5\u6a39\u7684\u6700\u5927\u6df1\u5ea6\u70ba 4) clf = RandomForestClassifier(n_estimators=10000, max_depth=10)  # \u8a13\u7df4\u6a21\u578b clf.fit(x_train, y_train)  # \u9810\u6e2c\u6e2c\u8a66\u96c6 y_pred = clf.predict(x_test) In\u00a0[\u00a0]: Copied! <pre>acc = metrics.accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", acc)\n#0.9736842105263158\n#0.9736842105263158\n</pre> acc = metrics.accuracy_score(y_test, y_pred) print(\"Accuracy: \", acc) #0.9736842105263158 #0.9736842105263158 In\u00a0[\u00a0]: Copied! <pre>print(iris.feature_names)\n</pre> print(iris.feature_names) In\u00a0[\u00a0]: Copied! <pre>print(\"Feature importance: \", clf.feature_importances_)\n#[0.06292474 0.02576675 0.40464774 0.50666078]\n#[0.10037212 0.03238696 0.43257994 0.43466098]\n</pre> print(\"Feature importance: \", clf.feature_importances_) #[0.06292474 0.02576675 0.40464774 0.50666078] #[0.10037212 0.03238696 0.43257994 0.43466098]"},{"location":"machine_learning/ch01/05_random_forest/","title":"\u4e94\u3001\u96a8\u6a5f\u68ee\u6797\u00b6","text":""},{"location":"machine_learning/ch01/05_random_forest/","title":"[\u7bc4\u4f8b\u91cd\u9ede]\u00b6","text":"<p>\u4e86\u89e3\u96a8\u6a5f\u68ee\u6797\u7684\u5efa\u6a21\u65b9\u6cd5\u53ca\u5176\u4e2d\u8d85\u53c3\u6578\u7684\u610f\u7fa9</p>"},{"location":"machine_learning/ch01/05_random_forest/#import","title":"import \u9700\u8981\u7684\u5957\u4ef6\u00b6","text":""},{"location":"machine_learning/ch01/05_random_forest/","title":"\u7df4\u7fd2\u00b6","text":"<ol> <li>\u8a66\u8457\u8abf\u6574 RandomForestClassifier(...) \u4e2d\u7684\u53c3\u6578\uff0c\u4e26\u89c0\u5bdf\u662f\u5426\u6703\u6539\u8b8a\u7d50\u679c\uff1f</li> <li>\u6539\u7528\u5176\u4ed6\u8cc7\u6599\u96c6 (boston, wine)\uff0c\u4e26\u8207\u56de\u6b78\u6a21\u578b\u8207\u6c7a\u7b56\u6a39\u7684\u7d50\u679c\u9032\u884c\u6bd4\u8f03</li> </ol>"},{"location":"machine_learning/ch01/06_gradient_boosting_machine/","title":"\u516d\u3001\u68af\u5ea6\u63d0\u5347\u6a5f","text":"In\u00a0[\u00a0]: Copied! <pre>from sklearn import datasets, metrics\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n</pre> from sklearn import datasets, metrics from sklearn.ensemble import GradientBoostingClassifier from sklearn.model_selection import train_test_split In\u00a0[\u00a0]: Copied! <pre># \u8b80\u53d6\u9cf6\u5c3e\u82b1\u8cc7\u6599\u96c6\niris = datasets.load_iris()\n\n# \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\nx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n\n# \u5efa\u7acb\u6a21\u578b\nclf = GradientBoostingClassifier()\n\n# \u8a13\u7df4\u6a21\u578b\nclf.fit(x_train, y_train)\n\n# \u9810\u6e2c\u6e2c\u8a66\u96c6\ny_pred = clf.predict(x_test)\n</pre> # \u8b80\u53d6\u9cf6\u5c3e\u82b1\u8cc7\u6599\u96c6 iris = datasets.load_iris()  # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6 x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)  # \u5efa\u7acb\u6a21\u578b clf = GradientBoostingClassifier()  # \u8a13\u7df4\u6a21\u578b clf.fit(x_train, y_train)  # \u9810\u6e2c\u6e2c\u8a66\u96c6 y_pred = clf.predict(x_test) In\u00a0[\u00a0]: Copied! <pre>acc = metrics.accuracy_score(y_test, y_pred)\nprint(\"Acuuracy: \", acc)\n</pre> acc = metrics.accuracy_score(y_test, y_pred) print(\"Acuuracy: \", acc) In\u00a0[\u00a0]: Copied! <pre>digits = datasets.load_digits()\n</pre> digits = datasets.load_digits()"},{"location":"machine_learning/ch01/06_gradient_boosting_machine/","title":"\u516d\u3001\u68af\u5ea6\u63d0\u5347\u6a5f\u00b6","text":""},{"location":"machine_learning/ch01/06_gradient_boosting_machine/#import","title":"import \u9700\u8981\u7684\u5957\u4ef6\u00b6","text":""},{"location":"machine_learning/ch01/06_gradient_boosting_machine/","title":"\u7df4\u7fd2\u00b6","text":"<p>\u76ee\u524d\u5df2\u7d93\u5b78\u904e\u8a31\u591a\u7684\u6a21\u578b\uff0c\u76f8\u4fe1\u5927\u5bb6\u5c0d\u6574\u9ad4\u6d41\u7a0b\u61c9\u8a72\u6bd4\u8f03\u638c\u63e1\u4e86\uff0c\u9019\u6b21\u7df4\u7fd2\u8acb\u6539\u7528\u624b\u5beb\u8fa8\u8b58\u8cc7\u6599\u96c6\uff0c\u6b65\u9a5f\u6d41\u7a0b\u90fd\u662f\u4e00\u6a23\u7684\uff0c\u8acb\u8a66\u8457\u81ea\u5df1\u64b0\u5beb\u7a0b\u5f0f\u78bc\u4f86\u5b8c\u6210\u6240\u6709\u6b65\u9a5f</p>"},{"location":"machine_learning/ch02/01_data_cleaning/","title":"Section 1 \u7d71\u8a08\u6307\u6a19\u5be6\u4f5c\u7bc4\u4f8b","text":""},{"location":"machine_learning/ch02/01_data_cleaning/#section-1","title":"Section 1 \u7d71\u8a08\u6307\u6a19\u5be6\u4f5c\u7bc4\u4f8b\u00b6","text":""},{"location":"machine_learning/ch02/01_data_cleaning/","title":"\u5e38\u898b\u65bc\u8ff4\u6b78\u554f\u984c\u7684\u8a55\u4f30\u6307\u6a19\u00b6","text":"<ul> <li>Mean Absolute Error (MAE)</li> <li>Mean Squared Error (MSE)</li> </ul>"},{"location":"machine_learning/ch02/01_data_cleaning/","title":"\u5e38\u898b\u65bc\u5206\u985e\u554f\u984c\u7684\u6307\u6a19\u00b6","text":"<ul> <li>Binary Cross Entropy (CE)</li> </ul>"},{"location":"machine_learning/ch02/01_data_cleaning/","title":"\u5f8c\u9762\u7684\u8ab2\u7a0b\u9084\u6703\u6709\u66f4\u8a73\u7d30\u7684\u8aaa\u660e\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"Section 1 \u7d71\u8a08\u6307\u6a19\u5be6\u4f5c\u7bc4\u4f8b","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>w = 3\nb = 0.5\n\nx_lin = np.linspace(0, 100, 101)\n\ny = (x_lin + np.random.randn(101) * 5) * w + b\n\nplt.plot(x_lin, y, 'b.', label = 'data points')\nplt.title(\"Assume we have data points\")\nplt.legend(loc = 2)\nplt.show()\n</pre> w = 3 b = 0.5  x_lin = np.linspace(0, 100, 101)  y = (x_lin + np.random.randn(101) * 5) * w + b  plt.plot(x_lin, y, 'b.', label = 'data points') plt.title(\"Assume we have data points\") plt.legend(loc = 2) plt.show() In\u00a0[\u00a0]: Copied! <pre>y_hat = x_lin * w + b\nplt.plot(x_lin, y, 'b.', label = 'data')\nplt.plot(x_lin, y_hat, 'r-', label = 'prediction')\nplt.title(\"Assume we have data points (And the prediction)\")\nplt.legend(loc = 2)\nplt.show()\n</pre> y_hat = x_lin * w + b plt.plot(x_lin, y, 'b.', label = 'data') plt.plot(x_lin, y_hat, 'r-', label = 'prediction') plt.title(\"Assume we have data points (And the prediction)\") plt.legend(loc = 2) plt.show() In\u00a0[\u00a0]: Copied! <pre>def mean_absolute_error(y, yp):\n\"\"\"\n    \u8a08\u7b97 MAE\n    Args:\n        - y: \u5be6\u969b\u503c\n        - yp: \u9810\u6e2c\u503c\n    Return:\n        - mae: MAE\n    \"\"\"\n    mae = MAE = sum(abs(y - yp)) / len(y)\n    return mae\n\nMAE = mean_absolute_error(y, y_hat)\nprint(\"The Mean absolute error is %.3f\" % (MAE))\n</pre> def mean_absolute_error(y, yp):     \"\"\"     \u8a08\u7b97 MAE     Args:         - y: \u5be6\u969b\u503c         - yp: \u9810\u6e2c\u503c     Return:         - mae: MAE     \"\"\"     mae = MAE = sum(abs(y - yp)) / len(y)     return mae  MAE = mean_absolute_error(y, y_hat) print(\"The Mean absolute error is %.3f\" % (MAE)) In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>===========================================================================================</p> <p>===========================================================================================</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\n</pre> import os import numpy as np import pandas as pd In\u00a0[\u00a0]: Copied! <pre>from google.colab import drive\ndrive.mount('/content/drive')\n</pre> from google.colab import drive drive.mount('/content/drive') In\u00a0[\u00a0]: Copied! <pre># \u8a2d\u5b9a data_path\ndir_data = '/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/110-2/20220421 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data'\n</pre> # \u8a2d\u5b9a data_path dir_data = '/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/110-2/20220421 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data' In\u00a0[\u00a0]: Copied! <pre>f_app = os.path.join(dir_data, 'application_train.csv')\nprint('Path of read in data: %s' % (f_app))\napp_train = pd.read_csv(f_app)\n</pre> f_app = os.path.join(dir_data, 'application_train.csv') print('Path of read in data: %s' % (f_app)) app_train = pd.read_csv(f_app) In\u00a0[\u00a0]: Copied! <pre># for example\n?pd.read_csv\n</pre> # for example ?pd.read_csv In\u00a0[\u00a0]: Copied! <pre>app_train.head()\n</pre> app_train.head() In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>===========================================================================================</p> <p>===========================================================================================</p> <p>===========================================================================================</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>data = {'weekday': ['Sun', 'Sun', 'Mon', 'Mon'],\n        'city': ['Austin', 'Dallas', 'Austin', 'Dallas'],\n        'visitor': [139, 237, 326, 456]}\n</pre> data = {'weekday': ['Sun', 'Sun', 'Mon', 'Mon'],         'city': ['Austin', 'Dallas', 'Austin', 'Dallas'],         'visitor': [139, 237, 326, 456]} In\u00a0[\u00a0]: Copied! <pre>visitors_1 = pd.DataFrame(data)\nvisitors_1\n</pre> visitors_1 = pd.DataFrame(data) visitors_1 In\u00a0[\u00a0]: Copied! <pre>with open(\"/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/110-2/20220421 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.txt\", 'r') as f:\n    data = f.readlines()\nprint(data)\n</pre> with open(\"/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/110-2/20220421 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.txt\", 'r') as f:     data = f.readlines() print(data) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ndata = []\nwith open(\"/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/110-2/20220421 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.txt\", 'r') as f:\n    for line in f:\n        line = line.replace('\\n', '').split(',') # \u5c07\u6bcf\u53e5\u6700\u5f8c\u7684 \\n \u53d6\u4ee3\u6210\u7a7a\u503c\u5f8c\uff0c\u518d\u4ee5\u9017\u865f\u65b7\u53e5\n        data.append(line)\ndata\n</pre> import pandas as pd  data = [] with open(\"/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/110-2/20220421 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.txt\", 'r') as f:     for line in f:         line = line.replace('\\n', '').split(',') # \u5c07\u6bcf\u53e5\u6700\u5f8c\u7684 \\n \u53d6\u4ee3\u6210\u7a7a\u503c\u5f8c\uff0c\u518d\u4ee5\u9017\u865f\u65b7\u53e5         data.append(line) data In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(data[1:])\ndf.columns = data[0]\ndf\n</pre> df = pd.DataFrame(data[1:]) df.columns = data[0] df In\u00a0[\u00a0]: Copied! <pre>import json\ndf.to_json('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example01.json')\n</pre> import json df.to_json('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example01.json') In\u00a0[\u00a0]: Copied! <pre># \u4e0a\u9762\u7684\u5b58\u5165\u65b9\u5f0f\uff0c\u6703\u5c07 column name \u505a\u70ba\u4e3b\u8981\u7684 key, row name \u505a\u70ba\u6b21\u8981\u7684 key\nwith open('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example01.json', 'r') as f:\n    j1 = json.load(f)\nj1\n</pre> # \u4e0a\u9762\u7684\u5b58\u5165\u65b9\u5f0f\uff0c\u6703\u5c07 column name \u505a\u70ba\u4e3b\u8981\u7684 key, row name \u505a\u70ba\u6b21\u8981\u7684 key with open('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example01.json', 'r') as f:     j1 = json.load(f) j1 In\u00a0[\u00a0]: Copied! <pre>df.set_index('id', inplace=True)\ndf\n</pre> df.set_index('id', inplace=True) df In\u00a0[\u00a0]: Copied! <pre>df.to_json('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example02.json', orient='index')\n</pre> df.to_json('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example02.json', orient='index') In\u00a0[\u00a0]: Copied! <pre>with open('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example02.json', 'r') as f:\n    j2 = json.load(f)\nj2\n</pre> with open('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example02.json', 'r') as f:     j2 = json.load(f) j2 In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n# \u5c07 data \u7684\u6578\u503c\u90e8\u5206\u8f49\u6210 numpy array\narray = np.array(data[1:])\narray\n</pre> import numpy as np # \u5c07 data \u7684\u6578\u503c\u90e8\u5206\u8f49\u6210 numpy array array = np.array(data[1:]) array In\u00a0[\u00a0]: Copied! <pre>np.save(arr=array, file='/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.npy')\n</pre> np.save(arr=array, file='/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.npy') In\u00a0[\u00a0]: Copied! <pre>array_back = np.load('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.npy')\narray_back\n</pre> array_back = np.load('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.npy') array_back In\u00a0[\u00a0]: Copied! <pre>import pickle\nwith open('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.pkl', 'wb') as f:\n    pickle.dump(file=f, obj=data)\n</pre> import pickle with open('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.pkl', 'wb') as f:     pickle.dump(file=f, obj=data) In\u00a0[\u00a0]: Copied! <pre>with open('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.pkl', 'rb') as f:\n    pkl_data = pickle.load(f)\npkl_data\n</pre> with open('/content/drive/MyDrive/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927\u8b1b\u5e2b/\u4e2d\u5927 - \u5929\u6c23\u8207\u4eba\u5de5\u667a\u6167/111-1/20221109 \u6a5f\u5668\u5b78\u7fd2\u5be6\u4f5c2 \u8cc7\u6599\u6e05\u7406\u6578\u64da\u524d\u8655\u7406/data/examples/example.pkl', 'rb') as f:     pkl_data = pickle.load(f) pkl_data In\u00a0[\u00a0]: Copied! <pre>cities = ['Austin', 'Dallas', 'Austin', 'Dallas']\nweekdays = ['Sun', 'Sun', 'Mon', 'Mon']\nvisitors = [139, 237, 326, 456]\n\nlist_labels = ['city', 'weekday', 'visitor']\nlist_cols = [cities, weekdays, visitors]\n\nzipped = list(zip(list_labels, list_cols))\n</pre> cities = ['Austin', 'Dallas', 'Austin', 'Dallas'] weekdays = ['Sun', 'Sun', 'Mon', 'Mon'] visitors = [139, 237, 326, 456]  list_labels = ['city', 'weekday', 'visitor'] list_cols = [cities, weekdays, visitors]  zipped = list(zip(list_labels, list_cols)) In\u00a0[\u00a0]: Copied! <pre>zipped\n</pre> zipped In\u00a0[\u00a0]: Copied! <pre>visitors_2 = pd.DataFrame(dict(zipped))\nvisitors_2\n</pre> visitors_2 = pd.DataFrame(dict(zipped)) visitors_2 In\u00a0[\u00a0]: Copied! <pre>visitors_1.groupby(by=\"weekday\")['visitor'].mean()\n</pre> visitors_1.groupby(by=\"weekday\")['visitor'].mean() In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>===========================================================================================</p> <p>===========================================================================================</p> <p>===========================================================================================</p> <p>===========================================================================================</p> <p>===========================================================================================</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt import numpy as np %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>import skimage.io as skio\nimg1 = skio.imread('data/examples/example.jpg')\nplt.imshow(img1)\nplt.show()\n</pre> import skimage.io as skio img1 = skio.imread('data/examples/example.jpg') plt.imshow(img1) plt.show() In\u00a0[\u00a0]: Copied! <pre>from PIL import Image\nimg2 = Image.open('data/examples/example.jpg') # \u9019\u6642\u5019\u9084\u662f PIL object\nimg2 = np.array(img2)\nplt.imshow(img2)\nplt.show()\n</pre> from PIL import Image img2 = Image.open('data/examples/example.jpg') # \u9019\u6642\u5019\u9084\u662f PIL object img2 = np.array(img2) plt.imshow(img2) plt.show() In\u00a0[\u00a0]: Copied! <pre>import cv2\nimg3 = cv2.imread('data/examples/example.jpg')\nplt.imshow(img3)\nplt.show()\n\nimg3 = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\nplt.imshow(img3)\nplt.show()\n</pre> import cv2 img3 = cv2.imread('data/examples/example.jpg') plt.imshow(img3) plt.show()  img3 = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB) plt.imshow(img3) plt.show() In\u00a0[\u00a0]: Copied! <pre>N_times = 1000\n</pre> N_times = 1000 In\u00a0[\u00a0]: Copied! <pre>%%timeit\nim = np.array([skio.imread('data/examples/example.jpg') for _ in range(N_times)])\n</pre> %%timeit im = np.array([skio.imread('data/examples/example.jpg') for _ in range(N_times)]) In\u00a0[\u00a0]: Copied! <pre>%%timeit\nim = np.array([np.array(Image.open('data/examples/example.jpg')) for _ in range(N_times)])\n</pre> %%timeit im = np.array([np.array(Image.open('data/examples/example.jpg')) for _ in range(N_times)]) In\u00a0[\u00a0]: Copied! <pre>%%timeit\nim = np.array([cv2.cvtColor(cv2.imread('data/examples/example.jpg'), cv2.COLOR_BGR2RGB) for _ in range(N_times)])\n</pre> %%timeit im = np.array([cv2.cvtColor(cv2.imread('data/examples/example.jpg'), cv2.COLOR_BGR2RGB) for _ in range(N_times)]) In\u00a0[\u00a0]: Copied! <pre>import scipy.io as sio\nsio.savemat(file_name='data/examples/example.mat', mdict={'img': img1})\n</pre> import scipy.io as sio sio.savemat(file_name='data/examples/example.mat', mdict={'img': img1}) In\u00a0[\u00a0]: Copied! <pre>mat_arr = sio.loadmat('data/examples/example.mat')\nprint(mat_arr.keys())\n</pre> mat_arr = sio.loadmat('data/examples/example.mat') print(mat_arr.keys()) In\u00a0[\u00a0]: Copied! <pre>mat_arr = mat_arr['img']\nprint(mat_arr.shape)\n</pre> mat_arr = mat_arr['img'] print(mat_arr.shape) In\u00a0[\u00a0]: Copied! <pre>plt.imshow(mat_arr)\nplt.show()\n</pre> plt.imshow(mat_arr) plt.show() In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport requests\ndata = []\ndata = requests.get('https://raw.githubusercontent.com/vashineyu/slides_and_others/master/tutorial/examples/imagenet_urls_examples.txt')\nfor i in data.content:\n    print(i)\n\n#    for line in f:\n#       line = line.replace('\\n', '').split(',') # \u5c07\u6bcf\u53e5\u6700\u5f8c\u7684 /n \u53d6\u4ee3\u6210\u7a7a\u503c\u5f8c\uff0c\u518d\u4ee5\u9017\u865f\u65b7\u53e5\n  #      data.append(line)\n</pre> import pandas as pd import requests data = [] data = requests.get('https://raw.githubusercontent.com/vashineyu/slides_and_others/master/tutorial/examples/imagenet_urls_examples.txt') for i in data.content:     print(i)  #    for line in f: #       line = line.replace('\\n', '').split(',') # \u5c07\u6bcf\u53e5\u6700\u5f8c\u7684 /n \u53d6\u4ee3\u6210\u7a7a\u503c\u5f8c\uff0c\u518d\u4ee5\u9017\u865f\u65b7\u53e5   #      data.append(line)"},{"location":"machine_learning/ch02/tmp/#section-1","title":"Section 1 \u7d71\u8a08\u6307\u6a19\u5be6\u4f5c\u7bc4\u4f8b\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"\u5e38\u898b\u65bc\u8ff4\u6b78\u554f\u984c\u7684\u8a55\u4f30\u6307\u6a19\u00b6","text":"<ul> <li>Mean Absolute Error (MAE)</li> <li>Mean Squared Error (MSE)</li> </ul>"},{"location":"machine_learning/ch02/tmp/","title":"\u5e38\u898b\u65bc\u5206\u985e\u554f\u984c\u7684\u6307\u6a19\u00b6","text":"<ul> <li>Binary Cross Entropy (CE)</li> </ul>"},{"location":"machine_learning/ch02/tmp/","title":"\u5f8c\u9762\u7684\u8ab2\u7a0b\u9084\u6703\u6709\u66f4\u8a73\u7d30\u7684\u8aaa\u660e\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"\u7df4\u7fd2\u6642\u9593\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#mean-square-error","title":"\u8acb\u5beb\u4e00\u500b\u51fd\u5f0f\u7528\u4f86\u8a08\u7b97 Mean Square Error\u00b6","text":"<p>$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}{(Y_i - \\hat{Y}_i)^2} $</p>"},{"location":"machine_learning/ch02/tmp/#hint","title":"Hint: \u5982\u4f55\u53d6\u5e73\u65b9\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#section-2","title":"Section 2\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"\u8b80\u53d6\u8cc7\u6599\u00b6","text":"<p>\u9996\u5148\uff0c\u6211\u5011\u7528 pandas \u8b80\u53d6\u6700\u4e3b\u8981\u7684\u8cc7\u6599 application_train.csv (\u8a18\u5f97\u5230 https://www.kaggle.com/c/home-credit-default-risk/data \u4e0b\u8f09)</p> <p>Note: <code>data/application_train.csv</code> \u8868\u793a <code>application_train.csv</code> \u8207\u8a72 <code>.ipynb</code> \u7684\u8cc7\u6599\u593e\u7d50\u69cb\u95dc\u4fc2\u5982\u4e0b</p> <pre><code>data\n    /application_train.csv\n20201021.ipynb\n</code></pre>"},{"location":"machine_learning/ch02/tmp/#pdread_csv","title":"\u7528 pd.read_csv \u4f86\u8b80\u53d6\u8cc7\u6599\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#note-jupyter-notebook","title":"Note: \u5728 jupyter notebook \u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>?</code> \u4f86\u8abf\u67e5\u51fd\u6578\u7684\u5b9a\u7fa9\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#head-5-row","title":"\u63a5\u4e0b\u4f86\u6211\u5011\u53ef\u4ee5\u7528 .head() \u9019\u500b\u51fd\u6578\u4f86\u89c0\u5bdf\u524d 5 row \u8cc7\u6599\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"\u7df4\u7fd2\u6642\u9593\u00b6","text":"<p>\u8cc7\u6599\u7684\u64cd\u4f5c\u6709\u5f88\u591a\uff0c\u672a\u4f86\u6703\u4ecb\u7d39\u5e38\u88ab\u4f7f\u7528\u5230\u7684\u64cd\u4f5c\uff0c\u5927\u5bb6\u4e0d\u59a8\u5148\u81ea\u884c\u60f3\u50cf\u4e00\u4e0b\uff0c\u7b2c\u4e00\u6b21\u770b\u5230\u8cc7\u6599\uff0c\u6211\u5011\u4e00\u822c\u6703\u60f3\u77e5\u9053\u4ec0\u9ebc\u8a0a\u606f\uff1f</p>"},{"location":"machine_learning/ch02/tmp/#ex-row-column","title":"Ex: \u5982\u4f55\u77e5\u9053\u8cc7\u6599\u7684 row \u6578\u4ee5\u53ca column \u6578\u3001\u6709\u4ec0\u9ebc\u6b04\u4f4d\u3001\u591a\u5c11\u6b04\u4f4d\u3001\u5982\u4f55\u622a\u53d6\u90e8\u5206\u7684\u8cc7\u6599\u7b49\u7b49\u00b6","text":"<p>\u6709\u4e86\u5c0d\u8cc7\u6599\u7684\u597d\u5947\u4e4b\u5f8c\uff0c\u6211\u5011\u53c8\u600e\u9ebc\u901a\u904e\u7a0b\u5f0f\u78bc\u4f86\u9054\u6210\u6211\u5011\u7684\u76ee\u7684\u5462\uff1f</p>"},{"location":"machine_learning/ch02/tmp/#google","title":"\u53ef\u53c3\u8003\u8a72\u57fa\u790e\u6559\u6750\u6216\u81ea\u884c google\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#eg","title":"e.g.\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#row-column","title":"\u8cc7\u6599\u7684 row \u6578\u4ee5\u53ca column \u6578\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"\u5217\u51fa\u6240\u6709\u6b04\u4f4d\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#pdiloc","title":"\u622a\u53d6\u90e8\u5206\u8cc7\u6599 pd.iloc[]\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#section-3","title":"Section 3\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#datafreme","title":"\u81ea\u5df1\u5efa\u7acb datafreme\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"\u65b9\u6cd5\u4e00\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#txt","title":"\u4f7f\u7528\u5167\u5efa\u529f\u80fd\u8b80\u53d6 txt \u6a94\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#txt-pandas-dataframe","title":"\u5c07 txt \u8f49\u6210 pandas dataframe\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#json","title":"\u5c07\u8cc7\u6599\u8f49\u6210 json \u6a94\u5f8c\u8f38\u51fa\u00b6","text":"<p>\u5c07 json \u8b80\u56de\u4f86\u5f8c\uff0c\u662f\u5426\u8207\u6211\u5011\u539f\u672c\u60f3\u8981\u5b58\u5165\u7684\u65b9\u5f0f\u4e00\u6a23? (\u4ee5 id \u70ba key)</p>"},{"location":"machine_learning/ch02/tmp/#npy","title":"\u5c07\u6a94\u6848\u5b58\u70ba npy \u6a94\u00b6","text":"<p>\u4e00\u500b\u5c08\u9580\u5132\u5b58 numpy array \u7684\u6a94\u6848\u683c\u5f0f \u4f7f\u7528 npy \u901a\u5e38\u53ef\u4ee5\u8b93\u4f60\u66f4\u5feb\u8b80\u53d6\u8cc7\u6599\u5594! \u5efa\u8b70\u95b1\u8b80</p>"},{"location":"machine_learning/ch02/tmp/#pickle","title":"Pickle\u00b6","text":"<p>\u5b58\u6210 pickle \u6a94 \u4ec0\u9ebc\u90fd\u5305\uff0c\u4ec0\u9ebc\u90fd\u4e0d\u5947\u602a\u7684 Pickle \u6bd4\u5982\u8aaa CIFAR10 \u7684\u8cc7\u6599\u96c6\u5c31\u662f\u7528 pickle \u5305\u7684\u5594!</p>"},{"location":"machine_learning/ch02/tmp/","title":"\u65b9\u6cd5\u4e8c\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"\u4e00\u500b\u7c21\u55ae\u4f8b\u5b50\u00b6","text":"<p>\u5047\u8a2d\u4f60\u60f3\u77e5\u9053\u5982\u679c\u5229\u7528 pandas \u8a08\u7b97\u4e0a\u8ff0\u8cc7\u6599\u4e2d\uff0c\u6bcf\u500b weekday \u7684\u5e73\u5747 visitor \u6578\u91cf\uff0c</p> <p>\u901a\u904e google \u4f60\u627e\u5230\u4e86 https://stackoverflow.com/questions/30482071/how-to-calculate-mean-values-grouped-on-another-column-in-pandas</p> <p>\u60f3\u8981\u6e2c\u8a66\u7684\u6642\u5019\u5c31\u53ef\u4ee5\u7528 visitors_1 \u9019\u500b\u53ea\u6709 4 \u7b46\u8cc7\u6599\u7684\u8cc7\u6599\u96c6\u4f86\u6e2c\u8a66\u7a0b\u5f0f\u78bc</p>"},{"location":"machine_learning/ch02/tmp/","title":"\u7df4\u7fd2\u6642\u9593\u00b6","text":"<p>\u5728\u5c0f\u91cf\u7684\u8cc7\u6599\u4e0a\uff0c\u6211\u5011\u7528\u773c\u775b\u5c31\u53ef\u4ee5\u770b\u5f97\u51fa\u4f86\u7a0b\u5f0f\u78bc\u662f\u5426\u6709\u8dd1\u51fa\u6211\u5011\u7406\u60f3\u4e2d\u7684\u7d50\u679c</p> <p>\u8acb\u5617\u8a66\u60f3\u50cf\u4e00\u500b\u4f60\u9700\u8981\u7684\u8cc7\u6599\u7d50\u69cb (\u88e1\u9762\u7684\u503c\u53ef\u4ee5\u662f\u96a8\u6a5f\u7684)\uff0c\u7136\u5f8c\u7528\u4e0a\u8ff0\u7684\u65b9\u6cd5\u628a\u5b83\u8b8a\u6210 pandas DataFrame</p>"},{"location":"machine_learning/ch02/tmp/#ex-dataframe","title":"Ex: \u751f\u6210\u4e00\u500b dataframe \u6709\u5169\u500b\u6b04\u4f4d\uff0c\u4e00\u500b\u662f\u570b\u5bb6\uff0c\u4e00\u500b\u662f\u4eba\u53e3\uff0c\u6c42\u4eba\u53e3\u6578\u6700\u591a\u7684\u570b\u5bb6\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#hints","title":"Hints: \u96a8\u6a5f\u7522\u751f\u6578\u503c\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"\u8b80\u53d6\u5716\u7247\u00b6","text":"<p>\u5e38\u898b\u7684\u5957\u4ef6:</p> <ol> <li>skimage</li> <li>PIL</li> <li>OpenCV</li> </ol>"},{"location":"machine_learning/ch02/tmp/#-1000","title":"\u6bd4\u8f03\u4e09\u7a2e\u958b\u5716\u65b9\u5f0f\u7684\u6642\u9593 - \u6bd4\u8f03\u8b80\u53d6 1000 \u6b21\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#mat","title":"\u5c07\u5f71\u50cf\u5b58\u6210 mat\u00b6","text":""},{"location":"machine_learning/ch02/tmp/","title":"\u7df4\u7fd2\u6642\u9593\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#1-1-txt","title":"1-1 \u8b80\u53d6 txt \u6a94\u00b6","text":"<ul> <li>\u8acb\u8b80\u53d6 text file</li> <li>\u61f6\u4eba\u8907\u88fd\u9023\u7d50: https://raw.githubusercontent.com/vashineyu/slides_and_others/master/tutorial/examples/imagenet_urls_examples.txt</li> </ul>"},{"location":"machine_learning/ch02/tmp/#1-2-txt-pandas-dataframe","title":"1-2 \u5c07\u6240\u63d0\u4f9b\u7684 txt \u8f49\u6210 pandas dataframe\u00b6","text":""},{"location":"machine_learning/ch02/tmp/#2-txt-data-frame-5","title":"2. \u5f9e\u6240\u63d0\u4f9b\u7684 txt \u4e2d\u7684\u9023\u7d50\u8b80\u53d6\u5716\u7247\uff0c\u8acb\u8b80\u53d6\u4e0a\u9762 data frame \u4e2d\u7684\u524d 5 \u5f35\u5716\u7247\u00b6","text":""},{"location":"python_basic/01_hello_world/","title":"Welcome to the Python World!","text":"In\u00a0[\u00a0]: Copied! <pre>print('Hello World')\n</pre> print('Hello World') In\u00a0[\u00a0]: Copied! <pre>strParam = 'Hello Python'\nprint(strParam)\n</pre> strParam = 'Hello Python' print(strParam)"},{"location":"python_basic/01_hello_world/#welcome-to-the-python-world","title":"Welcome to the Python World!\u00b6","text":""},{"location":"python_basic/02_flow_control/","title":"02 flow control","text":""},{"location":"python_basic/03_numpy/","title":"03 numpy","text":""},{"location":"python_basic/04_matplotlib/","title":"04 matplotlib","text":""},{"location":"python_basic/05_pandas/","title":"05 pandas","text":""}]}